# ml/train_improved.py

import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
import numpy as np
from pathlib import Path


class CryptoSentimentDataset(Dataset):
    """Dataset –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class ImprovedCryptoTrainer:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤
    """
    
    def __init__(self, data_path='ml/data/combined_dataset.csv'):
        self.data_path = Path(data_path)
        self.model_dir = Path('ml/models/crypto_sentiment')
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.label_map = {
            'negative': 0,
            'neutral': 1,
            'positive': 2
        }
        self.id2label = {v: k for k, v in self.label_map.items()}
        
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞...")
    
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π"""
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ {self.data_path}...")
        
        df = pd.read_csv(self.data_path)
        df = df[df['sentiment'].isin(['negative', 'neutral', 'positive'])]
        df['label'] = df['sentiment'].map(self.label_map)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"\nüìä –ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        print(df['sentiment'].value_counts())
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —á–µ—Ä–µ–∑ undersampling majority –∫–ª–∞—Å—Å–∞
        min_class_size = df['sentiment'].value_counts().min()
        
        df_balanced = pd.concat([
            df[df['sentiment'] == 'negative'].sample(n=min_class_size, random_state=42),
            df[df['sentiment'] == 'neutral'].sample(n=min_class_size, random_state=42),
            df[df['sentiment'] == 'positive'].sample(n=min_class_size, random_state=42),
        ]).sample(frac=1, random_state=42)  # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
        
        print(f"\n‚öñÔ∏è –ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:")
        print(df_balanced['sentiment'].value_counts())
        
        # –†–∞–∑–¥–µ–ª—è–µ–º
        train_df, temp_df = train_test_split(
            df_balanced, test_size=0.3, random_state=42, stratify=df_balanced['label']
        )
        val_df, test_df = train_test_split(
            temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']
        )
        
        print(f"\n‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ:")
        print(f"   Train: {len(train_df)}")
        print(f"   Val: {len(val_df)}")
        print(f"   Test: {len(test_df)}")
        
        return train_df, val_df, test_df
    
    def train(self, model_name='prajjwal1/bert-tiny', epochs=5, batch_size=16, learning_rate=2e-5):
        """–û–±—É—á–µ–Ω–∏–µ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π"""
        print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ: {model_name}")
        
        train_df, val_df, test_df = self.load_data()
        
        print(f"\nüì¶ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=3,
            id2label=self.id2label,
            label2id=self.label_map
        )
        
        # Datasets
        train_dataset = CryptoSentimentDataset(
            train_df['text'].values,
            train_df['label'].values,
            tokenizer
        )
        
        val_dataset = CryptoSentimentDataset(
            val_df['text'].values,
            val_df['label'].values,
            tokenizer
        )
        
        # Class weights –¥–ª—è loss function
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(train_df['label']),
            y=train_df['label']
        )
        class_weights = torch.tensor(class_weights, dtype=torch.float)
        
        print(f"\n‚öñÔ∏è Class weights: {class_weights.tolist()}")
        
        # Custom Trainer —Å weighted loss
        class WeightedTrainer(Trainer):
            def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
                labels = inputs.pop("labels")
                outputs = model(**inputs)
                logits = outputs.logits
                loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))
                loss = loss_fct(logits, labels)
                return (loss, outputs) if return_outputs else loss
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(self.model_dir / 'checkpoints'),
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            learning_rate=learning_rate,
            warmup_steps=100,
            weight_decay=0.01,
            logging_steps=50,
            eval_strategy='epoch',
            save_strategy='epoch',
            load_best_model_at_end=True,
            metric_for_best_model='accuracy',
        )
        
        def compute_metrics(pred):
            labels = pred.label_ids
            preds = pred.predictions.argmax(-1)
            acc = accuracy_score(labels, preds)
            return {'accuracy': acc}
        
        trainer = WeightedTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics,
        )
        
        print("\nüèãÔ∏è –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å...")
        trainer.train()
        
        # Test evaluation
        print("\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ test set...")
        test_dataset = CryptoSentimentDataset(
            test_df['text'].values,
            test_df['label'].values,
            tokenizer
        )
        
        predictions = trainer.predict(test_dataset)
        preds = predictions.predictions.argmax(-1)
        labels = predictions.label_ids
        
        print("\n" + "="*60)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê TEST SET:")
        print("="*60)
        print(f"\nAccuracy: {accuracy_score(labels, preds):.4f}")
        print("\nClassification Report:")
        print(classification_report(
            labels, preds,
            target_names=['negative', 'neutral', 'positive']
        ))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å...")
        model.save_pretrained(self.model_dir)
        tokenizer.save_pretrained(self.model_dir)
        
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        return model, tokenizer, accuracy_score(labels, preds)


if __name__ == '__main__':
    print("="*60)
    print("üéì –£–õ–£–ß–®–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("="*60)
    
    trainer = ImprovedCryptoTrainer()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º bert-tiny –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
    model, tokenizer, accuracy = trainer.train(
        model_name='distilbert-base-uncased',  # ‚Üê –ò–ó–ú–ï–ù–ò–¢–¨ (–±—ã–ª–æ bert-tiny)
        epochs=5,                               # ‚Üê –ò–ó–ú–ï–ù–ò–¢–¨ (–±—ã–ª–æ 8)
        batch_size=16,                          # ‚Üê –ò–ó–ú–ï–ù–ò–¢–¨ (–±—ã–ª–æ 32)
        learning_rate=2e-5,
    )
    
    print(f"\nüéâ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
