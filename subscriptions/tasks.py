import logging
from bot.telegram_bot import TG_TOKEN
from celery import shared_task
from django.db import transaction
from django.utils import timezone
from .models import CoinSnapshot, Subscription, CoinDailyStat, NewsArticle, NewsSentiment, PriceEvent, PricePrediction, DirectionPrediction
import numpy as np
import requests
from datetime import datetime, timedelta
import time
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.preprocessing import StandardScaler

import joblib



# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º
BASE_DIR = Path(__file__).resolve().parent.parent
ML_MODELS_DIR = BASE_DIR / 'ml' / 'models'

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
ML_MODELS_DIR.mkdir(parents=True, exist_ok=True)

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –º–æ–¥–µ–ª–µ–π
CLASSIFIER_MODEL_PATH = ML_MODELS_DIR / 'ml_classifier.pkl'
CLASSIFIER_SCALER_PATH = ML_MODELS_DIR / 'ml_classifier_scaler.pkl'
CLASSIFIER_FEATURES_PATH = ML_MODELS_DIR / 'classifier_features.pkl'

TRAINING_DATA_PATH = ML_MODELS_DIR / 'classification_data.csv'
MODEL_REPORT_PATH = ML_MODELS_DIR / 'model_report.json'
# ============================================
# –ó–∞–¥–∞—á–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –º–æ–Ω–µ—Ç–∞—Ö –∏–∑ CoinGecko
# ============================================

@shared_task
def update_coin_snapshots():
    url = "https://api.coingecko.com/api/v3/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": 20,
        "page": 1,
        "sparkline": "false"
    }
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        coins_data = response.json()

        with transaction.atomic():
            for coin in coins_data:
                CoinSnapshot.objects.update_or_create(
                    coingecko_id=coin["id"],
                    defaults={
                        "name": coin["name"],
                        "symbol": coin["symbol"],
                        "price": coin["current_price"],
                        "market_cap": coin.get("market_cap")  # –¥–æ–±–∞–≤–ª–µ–Ω–æ
                    }
                )

        print(f"[{datetime.now()}] –û–±–Ω–æ–≤–ª–µ–Ω–æ {len(coins_data)} –º–æ–Ω–µ—Ç")
        return f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {len(coins_data)} –º–æ–Ω–µ—Ç"

    except requests.RequestException as e:
        print(f"[{datetime.now()}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
        return f"–û—à–∏–±–∫–∞: {e}"



# ============================================
# # –ó–∞–¥–∞—á–∞ –¥–ª—è —Å–±–æ—Ä–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–Ω –ø–æ –¥–Ω—è–º
# ============================================

@shared_task
def collect_historical_prices(days=30):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
    """
    coins = CoinSnapshot.objects.all()[:10]  # –£–º–µ–Ω—å—à–∏–ª –¥–æ 10 –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    
    for coin in coins:
        try:
            url = f"https://api.coingecko.com/api/v3/coins/{coin.coingecko_id}/market_chart"
            params = {
                "vs_currency": "usd",
                "days": days,
                "interval": "daily"
            }
            
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            prices = data.get("prices", [])
            market_caps = data.get("market_caps", [])
            volumes = data.get("total_volumes", [])
            
            for i in range(len(prices)):
                timestamp = prices[i][0] / 1000
                date = datetime.utcfromtimestamp(timestamp).date()
                price = prices[i][1]
                market_cap = market_caps[i][1] if i < len(market_caps) else None
                volume = volumes[i][1] if i < len(volumes) else None
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
                prev_price = prices[i-1][1] if i > 0 else price
                price_change_percent = ((price - prev_price) / prev_price) * 100 if prev_price else 0
                
                CoinDailyStat.objects.update_or_create(
                    coin=coin,
                    date=date,
                    defaults={
                        "price": price,
                        "market_cap": market_cap,
                        "volume": volume,
                        "price_change_percent": price_change_percent
                    }
                )
            
            print(f"‚úÖ {coin.symbol.upper()} - –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(prices)} –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω")
            time.sleep(60)  # ‚Üê –ò–ó–ú–ï–ù–ò–õ –° 2 –ù–ê 5 –°–ï–ö–£–ù–î!
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {coin.symbol}: {e}")
    
    return f"–°–æ–±—Ä–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(coins)} –º–æ–Ω–µ—Ç"


# ============================================
# –ó–∞–¥–∞—á–∏ –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
# ============================================

@shared_task
def collect_historical_news(days=30):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π —á–µ—Ä–µ–∑ NewsAPI.org
    –° –†–ê–°–®–ò–†–ï–ù–ù–´–ú–ò –∑–∞–ø—Ä–æ—Å–∞–º–∏
    """
    from newsapi import NewsApiClient
    
    NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY')
    if not NEWSAPI_KEY:
        return "NEWSAPI_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω"
    
    newsapi = NewsApiClient(api_key=NEWSAPI_KEY)
    coins = CoinSnapshot.objects.order_by('-market_cap')[:10]
    total_articles = 0
    
    for coin in coins:
        try:
            # –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –í–ê–†–ò–ê–ù–¢–´ –ó–ê–ü–†–û–°–û–í –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç—ã
            queries = [
                f"{coin.name} cryptocurrency",
                f"{coin.symbol.upper()} price",
                f"{coin.name} {coin.symbol.upper()}",
                f"{coin.name} news",
                f"{coin.symbol.upper()} trading",
            ]
            
            from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            to_date = datetime.now().strftime('%Y-%m-%d')
            
            for query in queries:
                try:
                    all_articles = newsapi.get_everything(
                        q=query,
                        from_param=from_date,
                        to=to_date,
                        language='en',
                        sort_by='publishedAt',
                        page_size=100
                    )
                    
                    articles = all_articles.get('articles', [])
                    saved_count = 0
                    
                    for article in articles:
                        try:
                            if not article.get('url') or not article.get('title'):
                                continue
                            
                            published_str = article.get('publishedAt', '')
                            if published_str:
                                published_at = datetime.strptime(published_str, '%Y-%m-%dT%H:%M:%SZ')
                            else:
                                continue
                            
                            obj, created = NewsArticle.objects.get_or_create(
                                url=article['url'],
                                defaults={
                                    'coin': coin,
                                    'title': article['title'][:500],
                                    'description': article.get('description', '')[:1000] if article.get('description') else '',
                                    'source': article.get('source', {}).get('name', 'Unknown'),
                                    'published_at': published_at,
                                    'news_type': 'financial'
                                }
                            )
                            
                            if created:
                                saved_count += 1
                                
                        except Exception:
                            continue
                    
                    total_articles += saved_count
                    print(f"  '{query}' - {saved_count} –Ω–æ–≤—ã—Ö")
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–ª—è '{query}': {e}")
                    continue
            
            print(f"‚úÖ {coin.symbol.upper()} - –≤—Å–µ–≥–æ {total_articles} –Ω–æ–≤–æ—Å—Ç–µ–π")
            time.sleep(2)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {coin.symbol}: {e}")
            continue
    
    return f"–°–æ–±—Ä–∞–Ω–æ {total_articles} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"

@shared_task
def collect_political_news():
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –∫—Ä–∏–ø—Ç–æ—Ä—ã–Ω–æ–∫
    - –†–µ–≥—É–ª—è—Ü–∏–∏
    - –ì–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞
    - –ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞
    - –°—É–¥–µ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
    """
    from newsapi import NewsApiClient
    
    NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY')
    if not NEWSAPI_KEY:
        return "NEWSAPI_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω"
    
    newsapi = NewsApiClient(api_key=NEWSAPI_KEY)
    coins = CoinSnapshot.objects.order_by('-market_cap')[:10]
    
    # –ü–û–õ–ò–¢–ò–ß–ï–°–ö–ò–ï –¢–ï–ú–´ –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –∫—Ä–∏–ø—Ç—É
    political_queries = [
        # –†–µ–≥—É–ª—è—Ü–∏–∏
        'SEC cryptocurrency regulation',
        'crypto regulation bill congress',
        'European Union crypto regulation MiCA',
        'cryptocurrency ban government',
        
        # –°—É–¥–µ–±–Ω—ã–µ –¥–µ–ª–∞
        'Ripple SEC lawsuit',
        'cryptocurrency court case',
        'crypto exchange lawsuit',
        
        # –ú–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞
        'Federal Reserve interest rate crypto',
        'inflation cryptocurrency',
        'economic recession bitcoin',
        
        # –ì–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞
        'cryptocurrency sanctions Russia',
        'China cryptocurrency ban',
        'El Salvador bitcoin adoption',
        
        # –ü–æ–ª–∏—Ç–∏–∫–∞
        'Biden cryptocurrency policy',
        'Trump bitcoin',
        'crypto election campaign',
    ]
    
    total_articles = 0
    from_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    
    for query in political_queries:
        try:
            results = newsapi.get_everything(
                q=query,
                from_param=from_date,
                language='en',
                sort_by='relevancy',
                page_size=15  # –ø–æ 15 –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ç–µ–º—É
            )
            
            articles = results.get('articles', [])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –í–°–ï–• –º–æ–Ω–µ—Ç (–ø–æ–ª–∏—Ç–∏–∫–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–µ—Å—å —Ä—ã–Ω–æ–∫)
            for coin in coins:
                for article in articles:
                    try:
                        if not article.get('url') or not article.get('title'):
                            continue
                        
                        published_str = article.get('publishedAt', '')
                        if published_str:
                            published_at = datetime.strptime(published_str, '%Y-%m-%dT%H:%M:%SZ')
                        else:
                            continue
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É —á—Ç–æ —ç—Ç–æ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å
                        description = article.get('description', '') or ''
                        title = f"[POLITICAL] {article['title']}"[:500]
                        obj, created = NewsArticle.objects.get_or_create(
                            url=article['url'],
                            defaults={
                                'coin': coin,
                                'title': article['title'][:500],  # –ë–ï–ó [POLITICAL]!
                                'description': description[:1000],
                                'source': article.get('source', {}).get('name', 'Political News'),
                                'published_at': published_at,
                                'news_type': 'political'  # –î–û–ë–ê–í–ò–õ–ò
                            }
                        )

                        
                        if created:
                            total_articles += 1
                            
                    except Exception as e:
                        continue
            
            print(f"‚úÖ '{query[:40]}...' - {len(articles)} —Å—Ç–∞—Ç–µ–π")
            time.sleep(3)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è '{query}': {e}")
            continue
    
    return f"–°–æ–±—Ä–∞–Ω–æ {total_articles} –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"

@shared_task
def collect_market_news():
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –æ–±—â–µ—Ä—ã–Ω–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã
    –≠—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –±—É–¥—É—Ç —Å–≤—è–∑–∞–Ω—ã —Å–æ –í–°–ï–ú–ò –º–æ–Ω–µ—Ç–∞–º–∏
    """
    from newsapi import NewsApiClient
    
    NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY')
    if not NEWSAPI_KEY:
        return "NEWSAPI_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω"
    
    newsapi = NewsApiClient(api_key=NEWSAPI_KEY)
    
    # –û–±—â–µ—Ä—ã–Ω–æ—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    market_queries = [
        'cryptocurrency market crash',
        'crypto regulation SEC',
        'bitcoin ETF approval',
        'cryptocurrency adoption',
        'crypto market analysis',
    ]
    
    total_articles = 0
    coins = CoinSnapshot.objects.all()[:10]  
    
    for query in market_queries:
        try:
            from_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            
            all_articles = newsapi.get_everything(
                q=query,
                from_param=from_date,
                language='en',
                sort_by='relevancy',
                page_size=20  # –ø–æ 20 –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –∑–∞–ø—Ä–æ—Å
            )
            
            articles = all_articles.get('articles', [])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –ö–ê–ñ–î–û–ô –º–æ–Ω–µ—Ç—ã (–æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
            for coin in coins:
                for article in articles:
                    try:
                        if not article.get('url') or not article.get('title'):
                            continue
                        
                        published_str = article.get('publishedAt', '')
                        if published_str:
                            published_at = datetime.strptime(published_str, '%Y-%m-%dT%H:%M:%SZ')
                        else:
                            continue
                        obj, created = NewsArticle.objects.get_or_create(
                            url=article['url'],
                            defaults={
                                'coin': coin,
                                'title': article['title'][:500],
                                'description': description[:1000],
                                'source': article.get('source', {}).get('name', 'Market News'),
                                'published_at': published_at,
                                'news_type': 'market'  # –î–û–ë–ê–í–ò–õ–ò
                            }
                        )

                        
                        if created:
                            total_articles += 1
                            
                    except Exception as e:
                        continue
            
            print(f"‚úÖ Query '{query}' - —Å–æ–±—Ä–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
            time.sleep(2)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")
            continue
    
    return f"–°–æ–±—Ä–∞–Ω–æ {total_articles} –æ–±—â–µ—Ä—ã–Ω–æ—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"

@shared_task
def collect_crypto_news_extended():
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞–º
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    """
    from newsapi import NewsApiClient
    
    NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY')
    if not NEWSAPI_KEY:
        return "NEWSAPI_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω"
    
    newsapi = NewsApiClient(api_key=NEWSAPI_KEY)
    coins = CoinSnapshot.objects.all()[:10]  # 15 –º–æ–Ω–µ—Ç
    total_articles = 0
    
    # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫—Ä–∏–ø—Ç–æ-–∏—Å—Ç–æ—á–Ω–∏–∫–∏
    crypto_sources = [
        'crypto-coins-news',
        'techcrunch',
        'the-verge',
        'wired',
        'ars-technica'
    ]
    
    for coin in coins:
        try:
            # –ó–∞–ø—Ä–æ—Å 1: –û–±—â–∏–π –ø–æ–∏—Å–∫
            query = f'"{coin.name}" OR "{coin.symbol.upper()}" cryptocurrency'
            from_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            
            results = newsapi.get_everything(
                q=query,
                from_param=from_date,
                language='en',
                sort_by='relevancy',  # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                page_size=50
            )
            
            for article in results.get('articles', []):
                try:
                    if not article.get('url') or not article.get('title'):
                        continue
                        
                    published_at = datetime.strptime(
                        article['publishedAt'], 
                        '%Y-%m-%dT%H:%M:%SZ'
                    )
                    
                    obj, created = NewsArticle.objects.get_or_create(
                        url=article['url'],
                        defaults={
                            'coin': coin,
                            'title': article['title'][:500],
                            'description': article.get('description', '')[:1000] if article.get('description') else '',
                            'source': article.get('source', {}).get('name', 'Unknown'),
                            'published_at': published_at
                        }
                    )
                    
                    if created:
                        total_articles += 1
                        
                except Exception:
                    continue
            
            # –ó–∞–ø—Ä–æ—Å 2: –ü–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            try:
                source_results = newsapi.get_everything(
                    q=coin.name,
                    sources=','.join(crypto_sources),
                    from_param=from_date,
                    language='en',
                    page_size=30
                )
                
                for article in source_results.get('articles', []):
                    try:
                        if not article.get('url'):
                            continue
                            
                        published_at = datetime.strptime(
                            article['publishedAt'], 
                            '%Y-%m-%dT%H:%M:%SZ'
                        )
                        
                        NewsArticle.objects.get_or_create(
                            url=article['url'],
                            defaults={
                                'coin': coin,
                                'title': article['title'][:500],
                                'description': article.get('description', '')[:1000] if article.get('description') else '',
                                'source': article.get('source', {}).get('name', 'Unknown'),
                                'published_at': published_at
                            }
                        )
                    except Exception:
                        continue
                        
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è {coin.symbol}: {e}")
            
            print(f"‚úÖ {coin.symbol.upper()} - –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
            time.sleep(2)  # –ø–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–Ω–µ—Ç–∞–º–∏
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {coin.symbol}: {e}")
            continue
    
    return f"–°–æ–±—Ä–∞–Ω–æ {total_articles} –Ω–æ–≤–æ—Å—Ç–µ–π"




# –ê–ù–ê–õ–ò–ó –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò –ù–û–í–û–°–¢–ï–ô
# ============================================

@shared_task
def analyze_all_sentiment():
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç VADER –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    
    analyzer = SentimentIntensityAnalyzer()
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏ –ë–ï–ó –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    articles = NewsArticle.objects.filter(newssentiment__isnull=True)
    total_articles = articles.count()
    
    if total_articles == 0:
        return "–í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã"
    
    print(f"üí≠ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å {total_articles} –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    analyzed_count = 0
    
    for article in articles:
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            text = f"{article.title} {article.description or ''}"
            
            # VADER –∞–Ω–∞–ª–∏–∑
            scores = analyzer.polarity_scores(text)
            
            # compound: –æ—Ç -1 (–æ—á–µ–Ω—å –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ) –¥–æ +1 (–æ—á–µ–Ω—å –ø–æ–∑–∏—Ç–∏–≤–Ω–æ)
            sentiment_score = scores['compound']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if sentiment_score >= 0.05:
                label = 'positive'
            elif sentiment_score <= -0.05:
                label = 'negative'
            else:
                label = 'neutral'
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
            NewsSentiment.objects.create(
                article=article,
                sentiment_score=sentiment_score,
                sentiment_label=label,
                confidence=max(scores['pos'], scores['neg'], scores['neu'])
            )
            
            analyzed_count += 1
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 —Å—Ç–∞—Ç–µ–π
            if analyzed_count % 100 == 0:
                print(f"  –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {analyzed_count}/{total_articles}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç—å–∏ {article.id}: {e}")
            continue
    
    print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {analyzed_count} –∏–∑ {total_articles} —Å—Ç–∞—Ç–µ–π")
    return f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {analyzed_count} —Å—Ç–∞—Ç–µ–π"


# subscriptions/tasks.py

@shared_task
def setup_finbert():
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç FinBERT
    """
    try:
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        import torch
        
        print("üì• Downloading FinBERT model...")
        
        model_name = "ProsusAI/finbert"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        print("‚úÖ FinBERT loaded successfully")
        
        # –¢–µ—Å—Ç
        test_text = "Bitcoin surges to new all-time high as institutional adoption grows"
        
        inputs = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # FinBERT –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [positive, negative, neutral]
        labels = ['positive', 'negative', 'neutral']
        scores = probs[0].tolist()
        
        print(f"\nüì∞ Test: \"{test_text}\"")
        for label, score in zip(labels, scores):
            print(f"   {label}: {score:.3f}")
        
        return {'status': 'success', 'model': model_name}
        
    except ImportError:
        print("‚ùå transformers not installed")
        print("   Run: pip install transformers torch")
        return {'error': 'dependencies missing'}


def analyze_with_finbert(text):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é FinBERT
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    
    model_name = "ProsusAI/finbert"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    # Tokenize
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    
    # Predict
    with torch.no_grad():
        outputs = model(**inputs)
    
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    # FinBERT classes: positive, negative, neutral
    positive_score = probs[0][0].item()
    negative_score = probs[0][1].item()
    neutral_score = probs[0][2].item()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ [-1, 1] scale
    sentiment_score = positive_score - negative_score
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫—É
    max_idx = probs[0].argmax().item()
    labels = ['positive', 'negative', 'neutral']
    sentiment_label = labels[max_idx]
    
    confidence = probs[0][max_idx].item()
    
    return {
        'sentiment_score': sentiment_score,
        'sentiment_label': sentiment_label,
        'confidence': confidence
    }


@shared_task
def reanalyze_with_finbert():
    """
    –ü–µ—Ä–µ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å FinBERT (–ú–ï–î–õ–ï–ù–ù–û!)
    """
    articles = NewsArticle.objects.all()
    total = articles.count()
    
    print(f"üîÑ Re-analyzing {total} articles with FinBERT...")
    print("‚ö†Ô∏è  This will take ~30 minutes!")
    
    for i, article in enumerate(articles, 1):
        text = f"{article.title}. {article.description or ''}"
        
        result = analyze_with_finbert(text)
        
        NewsSentiment.objects.update_or_create(
            article=article,
            defaults={
                'sentiment_score': result['sentiment_score'],
                'sentiment_label': result['sentiment_label'],
                'confidence': result['confidence']
            }
        )
        
        if i % 50 == 0:
            print(f"   Processed {i}/{total} articles...")
    
    print(f"‚úÖ Re-analyzed with FinBERT")
    
    return {'total': total, 'method': 'finbert'}


# ============================================
# –ü–û–ò–°–ö –ê–ù–û–ú–ê–õ–ò–ô –í –¶–ï–ù–ê–•
# ============================================

@shared_task
def detect_all_anomalies(threshold_percent=1.5):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –∞–Ω–æ–º–∞–ª–∏–∏ (—Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è) —Ü–µ–Ω –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏—Ö —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - threshold_percent: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –≤ % –¥–ª—è —Å—á–∏—Ç–∞–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–µ–π
                         (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.5%)
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç—ã –±–µ—Ä–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ü–µ–Ω
    2. –ù–∞—Ö–æ–¥–∏–º –¥–Ω–∏ –≥–¥–µ —Ü–µ–Ω–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å –±–æ–ª–µ–µ —á–µ–º –Ω–∞ threshold_percent
    3. –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –±—ã–ª–æ –∑–∞ 3 –¥–Ω—è –î–û —ç—Ç–æ–≥–æ —Å–æ–±—ã—Ç–∏—è
    4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞–Ω–æ–º–∞–ª–∏—é (spike –∏–ª–∏ crash)
    """
    coins = CoinSnapshot.objects.all()
    anomaly_count = 0
    
    print(f"‚ö° –ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π (–ø–æ—Ä–æ–≥: {threshold_percent}%) –¥–ª—è {coins.count()} –º–æ–Ω–µ—Ç...")
    
    for coin in coins:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ü–µ–Ω, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–æ –¥–∞—Ç–µ
            stats = list(
                CoinDailyStat.objects.filter(coin=coin).order_by('date')
            )
            
            if len(stats) < 2:
                print(f"‚ö†Ô∏è  {coin.symbol.upper()} - –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö (< 2 –¥–Ω–µ–π)")
                continue
            
            coin_anomalies = 0
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–µ–Ω—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
            for i in range(1, len(stats)):
                try:
                    prev_stat = stats[i-1]
                    curr_stat = stats[i]
                    
                    prev_price = float(prev_stat.price)
                    curr_price = float(curr_stat.price)
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Ü–µ–Ω–∞ = 0
                    if prev_price == 0:
                        continue
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
                    change_percent = ((curr_price - prev_price) / prev_price) * 100
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥ –∞–Ω–æ–º–∞–ª–∏–∏
                    if abs(change_percent) > threshold_percent:
                        
                        # –°—á–∏—Ç–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ 3 –¥–Ω—è –î–û —Å–æ–±—ã—Ç–∏—è
                        news_period_start = curr_stat.date - timedelta(days=3)
                        news_period_end = curr_stat.date
                        
                        news_count = NewsArticle.objects.filter(
                            coin=coin,
                            published_at__date__range=[news_period_start, news_period_end]
                        ).count()
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–±—ã—Ç–∏—è
                        if change_percent > 0:
                            event_type = 'spike'  # —Ä–æ—Å—Ç
                        else:
                            event_type = 'crash'  # –ø–∞–¥–µ–Ω–∏–µ
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ
                        event, created = PriceEvent.objects.update_or_create(
                            coin=coin,
                            date=curr_stat.date,
                            defaults={
                                'event_type': event_type,
                                'price_change_percent': change_percent,
                                'price_before': prev_price,
                                'price_after': curr_price,
                                'is_anomaly': True,
                                'news_count': news_count
                            }
                        )
                        
                        if created:
                            anomaly_count += 1
                            coin_anomalies += 1
                
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–Ω—è {curr_stat.date}: {e}")
                    continue
            
            if coin_anomalies > 0:
                print(f"‚úÖ {coin.symbol.upper()} - –Ω–∞–π–¥–µ–Ω–æ {coin_anomalies} –∞–Ω–æ–º–∞–ª–∏–π")
            else:
                print(f"‚ÑπÔ∏è  {coin.symbol.upper()} - –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {coin.symbol}: {e}")
            continue
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {anomaly_count} –Ω–æ–≤—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π")
    return f"–ù–∞–π–¥–µ–Ω–æ {anomaly_count} –∞–Ω–æ–º–∞–ª–∏–π"


# ============================================
# –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–û–ú–ê–õ–ò–ô (–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û)
# ============================================

@shared_task
def get_anomalies_stats():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∞–Ω–æ–º–∞–ª–∏—è–º
    –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
    """
    from django.db.models import Count, Avg, Max, Min
    
    total_events = PriceEvent.objects.filter(is_anomaly=True).count()
    
    if total_events == 0:
        return "–ê–Ω–æ–º–∞–ª–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ detect_all_anomalies()"
    
    print("="*60)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–û–ú–ê–õ–ò–ô")
    print("="*60)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüéØ –í—Å–µ–≥–æ –∞–Ω–æ–º–∞–ª–∏–π: {total_events}")
    
    # –ü–æ —Ç–∏–ø–∞–º
    spikes = PriceEvent.objects.filter(event_type='spike', is_anomaly=True).count()
    crashes = PriceEvent.objects.filter(event_type='crash', is_anomaly=True).count()
    print(f"\nüìà –¢–∏–ø—ã:")
    print(f"  –†–æ—Å—Ç (spike):  {spikes} ({spikes/total_events*100:.1f}%)")
    print(f"  –ü–∞–¥–µ–Ω–∏–µ (crash): {crashes} ({crashes/total_events*100:.1f}%)")
    
    # –ü–æ –º–æ–Ω–µ—Ç–∞–º
    print(f"\nü™ô –ü–æ –º–æ–Ω–µ—Ç–∞–º:")
    by_coin = PriceEvent.objects.filter(is_anomaly=True).values(
        'coin__symbol', 'coin__name'
    ).annotate(
        count=Count('id'),
        avg_change=Avg('price_change_percent'),
        max_change=Max('price_change_percent'),
        min_change=Min('price_change_percent'),
        avg_news=Avg('news_count')
    ).order_by('-count')
    
    for item in by_coin:
        print(f"  {item['coin__symbol'].upper():8} - "
              f"{item['count']:2} —Å–æ–±—ã—Ç–∏–π, "
              f"—Å—Ä–µ–¥–Ω–µ–µ –∏–∑–º: {item['avg_change']:+.2f}%, "
              f"–Ω–æ–≤–æ—Å—Ç–µ–π: {item['avg_news']:.1f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    changes = PriceEvent.objects.filter(is_anomaly=True).aggregate(
        avg=Avg('price_change_percent'),
        max=Max('price_change_percent'),
        min=Min('price_change_percent')
    )
    print(f"\nüìä –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω:")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {changes['avg']:+.2f}%")
    print(f"  –ú–∞–∫—Å–∏–º—É–º: {changes['max']:+.2f}%")
    print(f"  –ú–∏–Ω–∏–º—É–º: {changes['min']:+.2f}%")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    news_stats = PriceEvent.objects.filter(is_anomaly=True).aggregate(
        avg_news=Avg('news_count'),
        max_news=Max('news_count'),
        min_news=Min('news_count')
    )
    print(f"\nüì∞ –ù–æ–≤–æ—Å—Ç–∏ (–∑–∞ 3 –¥–Ω—è –¥–æ —Å–æ–±—ã—Ç–∏—è):")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {news_stats['avg_news']:.1f}")
    print(f"  –ú–∞–∫—Å–∏–º—É–º: {news_stats['max_news']}")
    print(f"  –ú–∏–Ω–∏–º—É–º: {news_stats['min_news']}")
    
    # –¢–æ–ø-5 —Å–æ–±—ã—Ç–∏–π
    print(f"\nüî• –¢–æ–ø-5 —Å–∞–º—ã—Ö —Å–∏–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π:")
    top_events = PriceEvent.objects.filter(is_anomaly=True).select_related('coin').order_by(
        '-price_change_percent'
    )[:5]
    
    for event in top_events:
        print(f"  {event.coin.symbol.upper()} {event.date}: "
              f"{event.price_change_percent:+.2f}% "
              f"(–Ω–æ–≤–æ—Å—Ç–µ–π: {event.news_count})")
    
    print("\n" + "="*60)
    
    return {
        'total': total_events,
        'spikes': spikes,
        'crashes': crashes,
        'avg_change': changes['avg'],
        'avg_news': news_stats['avg_news']
    }


# ============================================
# –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï (–ó–∞–¥–∞—á–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω)
# ============================================
@shared_task
def prepare_daily_training_dataset_v2():
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏–Ω–∂–µ–Ω–µ—Ä–∏–µ–π –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    from datetime import timedelta
    import numpy as np
    import pandas as pd
    
    data = []
    
    for coin in CoinSnapshot.objects.all():
        print(f"Processing {coin.symbol}...")
        
        daily_stats = list(
            CoinDailyStat.objects
            .filter(coin=coin)
            .order_by('date')
            .values('date', 'price', 'volume', 'market_cap')
        )
        
        if len(daily_stats) < 8:
            continue
        
        for i in range(7, len(daily_stats) - 1):
            current_day = daily_stats[i]
            next_day = daily_stats[i + 1]
            
            # TARGET
            price_current = float(current_day['price'])
            price_next = float(next_day['price'])
            target_change_percent = ((price_next - price_current) / price_current) * 100
            
            # === –¶–ï–ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
            past_7_days = daily_stats[i-6:i+1]
            prices_7d = [float(d['price']) for d in past_7_days]
            volumes_7d = [float(d['volume']) for d in past_7_days]
            
            avg_price_7d = np.mean(prices_7d)
            volatility_7d = np.std(prices_7d)
            price_trend_7d = ((prices_7d[-1] - prices_7d[0]) / prices_7d[0]) * 100
            avg_volume_7d = np.mean(volumes_7d)
            
            # === –ù–û–í–û–°–¢–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò - –¢–ï–ö–£–©–ò–ô –ü–ï–†–ò–û–î (3 –¥–Ω—è) ===
            date_3d_ago = current_day['date'] - timedelta(days=3)
            
            news_current = NewsArticle.objects.filter(
                coin=coin,
                published_at__date__gte=date_3d_ago,
                published_at__date__lte=current_day['date']
            ).select_related('newssentiment')
            
            # === –ù–û–í–û–°–¢–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò - –ü–†–ï–î–´–î–£–©–ò–ô –ü–ï–†–ò–û–î (–¥–Ω–∏ -6 –¥–æ -3) ===
            date_6d_ago = current_day['date'] - timedelta(days=6)
            
            news_previous = NewsArticle.objects.filter(
                coin=coin,
                published_at__date__gte=date_6d_ago,
                published_at__date__lt=date_3d_ago
            ).select_related('newssentiment')
            
            # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
            news_count_current = news_current.count()
            sentiments_current = [
                n.newssentiment.sentiment_score 
                for n in news_current 
                if hasattr(n, 'newssentiment')
            ]
            avg_sentiment_current = np.mean(sentiments_current) if sentiments_current else 0
            positive_current = sum(1 for s in sentiments_current if s > 0.05)
            negative_current = sum(1 for s in sentiments_current if s < -0.05)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
            news_count_previous = news_previous.count()
            sentiments_previous = [
                n.newssentiment.sentiment_score 
                for n in news_previous 
                if hasattr(n, 'newssentiment')
            ]
            avg_sentiment_previous = np.mean(sentiments_previous) if sentiments_previous else 0
            positive_previous = sum(1 for s in sentiments_previous if s > 0.05)
            negative_previous = sum(1 for s in sentiments_previous if s < -0.05)
            
            # === –ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò: –ò–ó–ú–ï–ù–ï–ù–ò–ï –ù–û–í–û–°–¢–ù–û–ì–û –§–û–ù–ê ===
            news_volume_change = news_count_current - news_count_previous
            news_volume_ratio = news_count_current / news_count_previous if news_count_previous > 0 else 1.0
            
            sentiment_change = avg_sentiment_current - avg_sentiment_previous
            sentiment_acceleration = sentiment_change  # —Å–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            
            positive_change = positive_current - positive_previous
            negative_change = negative_current - negative_previous
            
            # –†–µ–∑–∫–∏–π –≤—Å–ø–ª–µ—Å–∫ –Ω–µ–≥–∞—Ç–∏–≤–∞ = –ø–ª–æ—Ö–æ–π —Å–∏–≥–Ω–∞–ª
            negative_spike = 1 if (negative_current > 5 and negative_change > 3) else 0
            
            # –†–µ–∑–∫–∏–π —Ä–æ—Å—Ç –ø–æ–∑–∏—Ç–∏–≤–∞ = —Ö–æ—Ä–æ—à–∏–π —Å–∏–≥–Ω–∞–ª
            positive_spike = 1 if (positive_current > 5 and positive_change > 3) else 0
            
            # === –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–ï –¶–ï–ù –ò –ù–û–í–û–°–¢–ï–ô ===
            # –ï—Å–ª–∏ —Ç—Ä–µ–Ω–¥ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –ò —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç–µ—Ç = —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
            price_sentiment_alignment = price_trend_7d * avg_sentiment_current
            
            # –î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è: —Ü–µ–Ω–∞ –ø–∞–¥–∞–µ—Ç, –Ω–æ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ = –≤–æ–∑–º–æ–∂–Ω—ã–π —Ä–∞–∑–≤–æ—Ä–æ—Ç
            divergence = 1 if (price_trend_7d < -1 and avg_sentiment_current > 0.1) else 0
            
            # === –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
            day_of_week = current_day['date'].weekday()
            month = current_day['date'].month
            
            # === –°–û–ë–ò–†–ê–ï–ú –î–ê–ù–ù–´–ï ===
            data.append({
                'coin': coin.symbol,
                'date': current_day['date'],
                'target': target_change_percent,
                
                # –¶–µ–Ω–æ–≤—ã–µ (4)
                'avg_price_7d': avg_price_7d,
                'volatility_7d': volatility_7d,
                'price_trend_7d': price_trend_7d,
                'avg_volume_7d': avg_volume_7d,
                
                # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ - –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ (3)
                'news_count_current': news_count_current,
                'avg_sentiment_current': avg_sentiment_current,
                'sentiment_std': np.std(sentiments_current) if sentiments_current else 0,
                
                # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ - –∏–∑–º–µ–Ω–µ–Ω–∏—è (6) - –ù–û–í–û–ï!
                'news_volume_change': news_volume_change,
                'sentiment_change': sentiment_change,
                'positive_change': positive_change,
                'negative_change': negative_change,
                'negative_spike': negative_spike,
                'positive_spike': positive_spike,
                
                # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ (2) - –ù–û–í–û–ï!
                'price_sentiment_alignment': price_sentiment_alignment,
                'divergence': divergence,
                
                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ (2)
                'day_of_week': day_of_week,
                'month': month
            })
    
    df = pd.DataFrame(data)
    
    # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    df['news_volume_change'] = df['news_volume_change'].clip(-50, 50)
    df['sentiment_change'] = df['sentiment_change'].clip(-1, 1)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    df.to_csv('subscriptions/training_data_v2.csv', index=False)
    
    print(f"‚úÖ Dataset created: {len(df)} samples")
    print(f"Target stats: mean={df['target'].mean():.2f}%, std={df['target'].std():.2f}%")
    print(f"\nNews features stats:")
    print(f"  news_volume_change: {df['news_volume_change'].mean():.1f} ¬± {df['news_volume_change'].std():.1f}")
    print(f"  sentiment_change: {df['sentiment_change'].mean():.3f} ¬± {df['sentiment_change'].std():.3f}")
    print(f"  negative_spike events: {df['negative_spike'].sum()}")
    print(f"  positive_spike events: {df['positive_spike'].sum()}")
    
    return {
        'total_samples': len(df),
        'coins': df['coin'].nunique(),
        'target_mean': float(df['target'].mean()),
        'target_std': float(df['target'].std())
    }


@shared_task
def train_prediction_model_v4():
    """
    –ú–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    """
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import GradientBoostingRegressor
    from sklearn.metrics import mean_absolute_error, r2_score
    import joblib
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ù–û–í–´–ô –¥–∞—Ç–∞—Å–µ—Ç
    df = pd.read_csv('subscriptions/training_data_v2.csv')
    
    print(f"üìä Dataset: {len(df)} samples")
    
    # === –í–´–ë–ò–†–ê–ï–ú –ü–†–ò–ó–ù–ê–ö–ò ===
    feature_cols = [
        # –¶–µ–Ω–æ–≤—ã–µ (4)
        'price_trend_7d',
        'volatility_7d',
        'avg_volume_7d',
        'avg_price_7d',
        
        # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ (8) - –û–°–ù–û–í–ù–û–ô –§–û–ö–£–°!
        'news_volume_change',      # –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        'sentiment_change',         # –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        'positive_change',          # —Ä–æ—Å—Ç –ø–æ–∑–∏—Ç–∏–≤–∞
        'negative_change',          # —Ä–æ—Å—Ç –Ω–µ–≥–∞—Ç–∏–≤–∞
        'negative_spike',           # –≤—Å–ø–ª–µ—Å–∫ –Ω–µ–≥–∞—Ç–∏–≤–∞
        'positive_spike',           # –≤—Å–ø–ª–µ—Å–∫ –ø–æ–∑–∏—Ç–∏–≤–∞
        'price_sentiment_alignment', # —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ü–µ–Ω—ã –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        'divergence',               # —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ
    ]
    
    print(f"üéØ Using {len(feature_cols)} features")
    print(f"   - Price: 4 features")
    print(f"   - News: 8 features (dynamic)")
    
    X = df[feature_cols]
    y = df['target']
    
    # Temporal split
    df_sorted = df.sort_values('date')
    split_idx = int(len(df_sorted) * 0.8)
    
    train_df = df_sorted.iloc[:split_idx]
    test_df = df_sorted.iloc[split_idx:]
    
    X_train = train_df[feature_cols]
    y_train = train_df['target']
    X_test = test_df[feature_cols]
    y_test = test_df['target']
    
    print(f"\nüì¶ Train: {len(train_df)} samples")
    print(f"üì¶ Test: {len(test_df)} samples")
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # === –ú–û–î–ï–õ–¨ –° –ë–û–õ–¨–®–ï–ô –ì–õ–£–ë–ò–ù–û–ô –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π ===
    model = GradientBoostingRegressor(
        n_estimators=50,
        learning_rate=0.05,
        max_depth=4,           # –≥–ª—É–±–∂–µ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
        min_samples_split=20,
        min_samples_leaf=10,
        subsample=0.8,
        max_features='sqrt',   # —Å–ª—É—á–∞–π–Ω—ã–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        random_state=42,
        verbose=0
    )
    
    print("\nüîß Training model...")
    model.fit(X_train_scaled, y_train)
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º
    train_predictions = model.predict(X_train_scaled)
    test_predictions = model.predict(X_test_scaled)
    
    train_r2 = r2_score(y_train, train_predictions)
    test_r2 = r2_score(y_test, test_predictions)
    train_mae = mean_absolute_error(y_train, train_predictions)
    test_mae = mean_absolute_error(y_test, test_predictions)
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n" + "="*60)
    print("üìà MODEL PERFORMANCE")
    print("="*60)
    print(f"Train R¬≤:  {train_r2:>7.4f}")
    print(f"Test R¬≤:   {test_r2:>7.4f}  {'‚úÖ' if test_r2 > 0 else '‚ùå'}")
    print(f"Train MAE: {train_mae:>6.2f}%")
    print(f"Test MAE:  {test_mae:>6.2f}%")
    print(f"Overfitting gap: {train_r2 - test_r2:.4f}")
    
    print("\n" + "="*60)
    print("üîù FEATURE IMPORTANCE")
    print("="*60)
    for idx, row in feature_importance.iterrows():
        bar = "‚ñà" * int(row['importance'] * 100)
        category = "üí∞" if any(p in row['feature'] for p in ['price', 'volume', 'volatility']) else "üì∞"
        print(f"{category} {row['feature']:.<35} {row['importance']*100:>5.1f}% {bar}")
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
    price_features = ['avg_price_7d', 'volatility_7d', 'price_trend_7d', 'avg_volume_7d']
    news_features = [f for f in feature_cols if f not in price_features]
    
    price_importance = feature_importance[feature_importance['feature'].isin(price_features)]['importance'].sum()
    news_importance = feature_importance[feature_importance['feature'].isin(news_features)]['importance'].sum()
    
    print("\n" + "="*60)
    print("üìä FEATURE GROUPS")
    print("="*60)
    print(f"üí∞ Price features:  {price_importance*100:>5.1f}%")
    print(f"üì∞ News features:   {news_importance*100:>5.1f}%")
    
    # –¢–æ–ø-3 –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞
    news_importance_df = feature_importance[feature_importance['feature'].isin(news_features)]
    if not news_importance_df.empty:
        print(f"\nüì∞ Top news features:")
        for idx, row in news_importance_df.head(3).iterrows():
            print(f"   {row['feature']}: {row['importance']*100:.1f}%")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    joblib.dump(model, 'subscriptions/ml_model.pkl')
    joblib.dump(scaler, 'subscriptions/ml_scaler.pkl')
    joblib.dump(feature_cols, 'subscriptions/feature_columns.pkl')
    
    print("\n‚úÖ Model saved successfully")
    
    return {
        'train_r2': float(train_r2),
        'test_r2': float(test_r2),
        'train_mae': float(train_mae),
        'test_mae': float(test_mae),
        'price_importance': float(price_importance),
        'news_importance': float(news_importance),
        'feature_importance': feature_importance.to_dict('records')
    }


@shared_task
def predict_price_change(coin_symbol):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –±—É–¥—É—â–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–Ω–µ—Ç—ã
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    
    Args:
        coin_symbol: —Å–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'btc')
    
    Returns:
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –≤ %
    """
    import pickle
    import os
    import numpy as np
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model_path = os.path.join(os.path.dirname(__file__), 'ml_model.pkl')
    
    if not os.path.exists(model_path):
        raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –ó–∞–ø—É—Å—Ç–∏—Ç–µ train_prediction_model()")
    
    with open(model_path, 'rb') as f:
        model_data = pickle.load(f)
    
    model = model_data['model']
    scaler = model_data['scaler']
    feature_columns = model_data['feature_columns']
    
    # –ü–æ–ª—É—á–∞–µ–º –º–æ–Ω–µ—Ç—É
    try:
        coin = CoinSnapshot.objects.get(symbol=coin_symbol.lower())
    except CoinSnapshot.DoesNotExist:
        raise Exception(f"–ú–æ–Ω–µ—Ç–∞ {coin_symbol} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—É—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ prepare_training_dataset)
    today = datetime.now().date()
    
    # –¶–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π)
    period_start = today - timedelta(days=7)
    price_stats = CoinDailyStat.objects.filter(
        coin=coin,
        date__range=[period_start, today]
    ).order_by('date')
    
    if price_stats.count() < 3:
        raise Exception(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {coin_symbol}")
    
    prices = [float(s.price) for s in price_stats]
    avg_price_7d = np.mean(prices)
    volatility_7d = np.std(prices)
    price_trend_7d = (prices[-1] - prices[0]) / prices[0] * 100
    
    # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è)
    news_period_start = today - timedelta(days=3)
    news = NewsArticle.objects.filter(
        coin=coin,
        published_at__date__range=[news_period_start, today]
    ).prefetch_related('newssentiment')
    
    news_count_3d = news.count()
    
    sentiments = []
    for article in news:
        if hasattr(article, 'newssentiment'):
            sentiments.append(article.newssentiment.sentiment_score)
    
    if sentiments:
        avg_sentiment = np.mean(sentiments)
        sentiment_std = np.std(sentiments)
        positive_ratio = len([s for s in sentiments if s > 0.1]) / len(sentiments)
        negative_ratio = len([s for s in sentiments if s < -0.1]) / len(sentiments)
        positive_count = len([s for s in sentiments if s > 0.1])
        negative_count = len([s for s in sentiments if s < -0.1])
        neutral_count = len([s for s in sentiments if -0.1 <= s <= 0.1])
    else:
        avg_sentiment = 0
        sentiment_std = 0
        positive_ratio = 0
        negative_ratio = 0
        positive_count = 0
        negative_count = 0
        neutral_count = 0
    
    news_per_day = news_count_3d / 3.0
    news_spike = 1 if news_count_3d > 50 else 0
    
    political_count = news.filter(news_type='political').count()
    financial_count = news.filter(news_type='financial').count()
    political_ratio = political_count / news_count_3d if news_count_3d > 0 else 0
    
    political_sentiments = []
    for article in news.filter(news_type='political'):
        if hasattr(article, 'newssentiment'):
            political_sentiments.append(article.newssentiment.sentiment_score)
    avg_political_sentiment = np.mean(political_sentiments) if political_sentiments else 0
    
    day_of_week = today.weekday()
    month = today.month
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = {
        'avg_price_7d': avg_price_7d,
        'volatility_7d': volatility_7d,
        'price_trend_7d': price_trend_7d,
        'news_count_3d': news_count_3d,
        'news_per_day': news_per_day,
        'news_spike': news_spike,
        'avg_sentiment': avg_sentiment,
        'sentiment_std': sentiment_std,
        'positive_ratio': positive_ratio,
        'negative_ratio': negative_ratio,
        'positive_count': positive_count,
        'negative_count': negative_count,
        'neutral_count': neutral_count,
        'political_count': political_count,
        'financial_count': financial_count,
        'political_ratio': political_ratio,
        'avg_political_sentiment': avg_political_sentiment,
        'day_of_week': day_of_week,
        'month': month
    }
    
    X = np.array([[features[col] for col in feature_columns]])
    X_scaled = scaler.transform(X)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    prediction = model.predict(X_scaled)[0]
    
    return {
        'coin': coin_symbol.upper(),
        'predicted_change': round(prediction, 2),
        'current_price': float(coin.price),
        'news_count': news_count_3d,
        'avg_sentiment': round(avg_sentiment, 2)
    }

# ============================================
# –ú–ê–®–ò–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï (–ó–∞–¥–∞—á–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞)
# ============================================
@shared_task
def prepare_classification_dataset():
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ml/models/classification_data.csv
    """
    from datetime import timedelta
    import numpy as np
    import pandas as pd
    
    data = []
    
    for coin in CoinSnapshot.objects.all():
        print(f"Processing {coin.symbol}...")
        
        daily_stats = list(
            CoinDailyStat.objects
            .filter(coin=coin)
            .order_by('date')
            .values('date', 'price', 'volume', 'market_cap')
        )
        
        if len(daily_stats) < 8:
            continue
        
        for i in range(7, len(daily_stats) - 1):
            current_day = daily_stats[i]
            next_day = daily_stats[i + 1]
            
            # TARGET: 0 = down, 1 = up
            price_current = float(current_day['price'])
            price_next = float(next_day['price'])
            price_change_percent = ((price_next - price_current) / price_current) * 100
            
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —à—É–º (<0.5%)
            if abs(price_change_percent) < 0.5:
                continue
            
            target = 1 if price_change_percent > 0 else 0
            
            # [... –∫–æ–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞–∫–∏–º –∂–µ ...]
            # (–≤—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è price, news –∏ —Ç.–¥.)
            
            data.append({
                'coin': coin.symbol,
                'date': current_day['date'],
                'target': target,
                'price_change_percent': price_change_percent,
                
                'price_trend_7d': price_trend_7d,
                'volatility_7d': volatility_7d,
                'avg_volume_7d': avg_volume_7d,
                'avg_price_7d': avg_price_7d,
                'news_volume_change': float(news_volume_change),
                'sentiment_change': float(sentiment_change),
                'positive_change': float(positive_change),
                'negative_change': float(negative_change),
                'negative_spike': float(negative_spike),
                'positive_spike': float(positive_spike),
                'price_sentiment_alignment': float(price_sentiment_alignment),
                'divergence': float(divergence),
            })
    
    df = pd.DataFrame(data)
    
    up_count = (df['target'] == 1).sum()
    down_count = (df['target'] == 0).sum()
    
    print(f"\n‚úÖ Dataset created: {len(df)} samples")
    print(f"üìä Class distribution:")
    print(f"   UP (1):   {up_count} ({up_count/len(df)*100:.1f}%)")
    print(f"   DOWN (0): {down_count} ({down_count/len(df)*100:.1f}%)")
    
    # –°–û–•–†–ê–ù–Ø–ï–ú –í ml/models/
    df.to_csv(TRAINING_DATA_PATH, index=False)
    print(f"üíæ Saved to: {TRAINING_DATA_PATH}")
    
    return {
        'total_samples': len(df),
        'up_count': int(up_count),
        'down_count': int(down_count),
        'saved_to': str(TRAINING_DATA_PATH)
    }


@shared_task
def train_classification_model_v2():
    """
    –û–±—É—á–∞–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ml/models/
    """
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix
    import joblib
    
    # –ó–ê–ì–†–£–ñ–ê–ï–ú –ò–ó ml/models/
    print(f"üìÇ Loading data from: {TRAINING_DATA_PATH}")
    df = pd.read_csv(TRAINING_DATA_PATH)
    
    print(f"üìä Dataset: {len(df)} samples")
    
    feature_cols = [
        'price_trend_7d', 'volatility_7d', 'avg_volume_7d', 'avg_price_7d',
        'sentiment_change', 'price_sentiment_alignment',
    ]
    
    print(f"üéØ Using {len(feature_cols)} features (reduced from 12)")
    
    X = df[feature_cols]
    y = df['target']
    
    # Temporal split
    df_sorted = df.sort_values('date')
    split_idx = int(len(df_sorted) * 0.8)
    
    train_df = df_sorted.iloc[:split_idx]
    test_df = df_sorted.iloc[split_idx:]
    
    X_train = train_df[feature_cols]
    y_train = train_df['target']
    X_test = test_df[feature_cols]
    y_test = test_df['target']
    
    print(f"\nüì¶ Train: {len(train_df)} samples")
    print(f"   UP: {(train_df['target']==1).sum()}, DOWN: {(train_df['target']==0).sum()}")
    print(f"üì¶ Test: {len(test_df)} samples")
    print(f"   UP: {(test_df['target']==1).sum()}, DOWN: {(test_df['target']==0).sum()}")
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –ú–æ–¥–µ–ª—å
    model = GradientBoostingClassifier(
        n_estimators=30,
        learning_rate=0.1,
        max_depth=3,
        min_samples_split=30,
        min_samples_leaf=15,
        subsample=0.7,
        max_features='sqrt',
        random_state=42,
        verbose=0
    )
    
    print("\nüîß Training simplified classifier...")
    model.fit(X_train_scaled, y_train)
    
    # –û—Ü–µ–Ω–∫–∞
    train_pred = model.predict(X_train_scaled)
    test_pred = model.predict(X_test_scaled)
    
    train_pred_proba = model.predict_proba(X_train_scaled)[:, 1]
    test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    train_acc = accuracy_score(y_train, train_pred)
    test_acc = accuracy_score(y_test, test_pred)
    
    train_auc = roc_auc_score(y_train, train_pred_proba)
    test_auc = roc_auc_score(y_test, test_pred_proba)
    
    print("\n" + "="*60)
    print("üìà CLASSIFICATION PERFORMANCE")
    print("="*60)
    print(f"Train Accuracy: {train_acc:.4f} ({train_acc*100:.1f}%)")
    print(f"Test Accuracy:  {test_acc:.4f} ({test_acc*100:.1f}%)  {'‚úÖ' if test_acc > 0.52 else '‚ö†Ô∏è'}")
    print(f"Train AUC-ROC:  {train_auc:.4f}")
    print(f"Test AUC-ROC:   {test_auc:.4f}  {'‚úÖ' if test_auc > 0.55 else '‚ö†Ô∏è'}")
    print(f"\nüìä Comparison:")
    print(f"   Baseline (random):     50.0%")
    print(f"   Your model:           {test_acc*100:.1f}%")
    print(f"   Improvement:          +{(test_acc - 0.5)*100:.1f}%")
    print(f"   Overfitting gap:      {(train_acc - test_acc)*100:.1f}%  {'‚úÖ' if (train_acc - test_acc) < 0.15 else '‚ö†Ô∏è'}")
    
    # Confusion matrix
    cm = confusion_matrix(y_test, test_pred)
    print("\n" + "="*60)
    print("üìã CONFUSION MATRIX (Test Set)")
    print("="*60)
    print(f"                Predicted")
    print(f"              DOWN    UP")
    print(f"Actual DOWN    {cm[0][0]:3d}   {cm[0][1]:3d}")
    print(f"       UP      {cm[1][0]:3d}   {cm[1][1]:3d}")
    
    # Classification report
    print("\n" + "="*60)
    print("üìã DETAILED METRICS")
    print("="*60)
    print(classification_report(y_test, test_pred, target_names=['DOWN', 'UP']))
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n" + "="*60)
    print("üîù FEATURE IMPORTANCE")
    print("="*60)
    for idx, row in feature_importance.iterrows():
        bar = "‚ñà" * int(row['importance'] * 100)
        category = "üí∞" if any(p in row['feature'] for p in ['price', 'volume', 'volatility']) else "üì∞"
        print(f"{category} {row['feature']:.<35} {row['importance']*100:>5.1f}% {bar}")
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
    price_features = ['avg_price_7d', 'volatility_7d', 'price_trend_7d', 'avg_volume_7d']
    news_features = [f for f in feature_cols if f not in price_features]
    
    price_importance = feature_importance[feature_importance['feature'].isin(price_features)]['importance'].sum()
    news_importance = feature_importance[feature_importance['feature'].isin(news_features)]['importance'].sum()
    
    print("\n" + "="*60)
    print("üìä FEATURE GROUPS")
    print("="*60)
    print(f"üí∞ Price features:  {price_importance*100:>5.1f}%")
    print(f"üì∞ News features:   {news_importance*100:>5.1f}%")
    
    # –°–û–•–†–ê–ù–Ø–ï–ú –í ml/models/
    joblib.dump(model, CLASSIFIER_MODEL_PATH)
    joblib.dump(scaler, CLASSIFIER_SCALER_PATH)
    joblib.dump(feature_cols, CLASSIFIER_FEATURES_PATH)
    
    print("\n" + "="*60)
    print("üíæ MODEL SAVED")
    print("="*60)
    print(f"   Model:    {CLASSIFIER_MODEL_PATH}")
    print(f"   Scaler:   {CLASSIFIER_SCALER_PATH}")
    print(f"   Features: {CLASSIFIER_FEATURES_PATH}")
    
    return {
        'train_acc': float(train_acc),
        'test_acc': float(test_acc),
        'train_auc': float(train_auc),
        'test_auc': float(test_auc),
        'improvement': float((test_acc - 0.5) * 100),
        'overfitting_gap': float((train_acc - test_acc) * 100),
        'price_importance': float(price_importance),
        'news_importance': float(news_importance),
        'confusion_matrix': cm.tolist(),
        'saved_to': {
            'model': str(CLASSIFIER_MODEL_PATH),
            'scaler': str(CLASSIFIER_SCALER_PATH),
            'features': str(CLASSIFIER_FEATURES_PATH)
        }
    }



# ============================================
# –ó–ê–î–ê–ß–ò –û–ë–ù–û–í–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–• –î–õ–Ø –ü–†–û–ì–ù–û–ó–ê
# ============================================
# subscriptions/tasks.py

@shared_task
def update_daily_data():
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∑–∞ –≤—á–µ—Ä–∞—à–Ω–∏–π –¥–µ–Ω—å:
    1. –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
    2. –û–±–Ω–æ–≤–ª—è–µ—Ç –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç
    """
    from django.utils import timezone
    
    print(f"üîÑ Updating daily data at {timezone.now()}")
    
    # 1. –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ —Ü–µ–Ω—ã (—Å–Ω–∞–ø—à–æ—Ç—ã)
    update_coin_snapshots()
    
    # 2. –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å
    # (CoinGecko –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–Ω–µ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–æ—ç—Ç–æ–º—É –±–µ—Ä–µ–º 2 –¥–Ω—è —á—Ç–æ–±—ã —Ç–æ—á–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –≤—á–µ—Ä–∞)
    collect_historical_prices(days=2)
    
    # 3. –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
    collect_recent_news()
    
    # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    analyze_all_sentiment()
    
    print("‚úÖ Daily data updated")
    
    return {'status': 'success', 'timestamp': timezone.now().isoformat()}

@shared_task
def collect_recent_news():
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞ –¥–ª—è –≤—Å–µ—Ö –º–æ–Ω–µ—Ç
    """

    
    NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY')
    if not NEWSAPI_KEY:
        print("‚ö†Ô∏è NEWSAPI_KEY not set")
        return
    
    yesterday = (timezone.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    for coin in CoinSnapshot.objects.all():
        query = f"{coin.name} OR {coin.symbol}"
        
        try:
            response = requests.get(
                'https://newsapi.org/v2/everything',
                params={
                    'q': query,
                    'from': yesterday,
                    'sortBy': 'publishedAt',
                    'language': 'en',
                    'apiKey': NEWSAPI_KEY
                },
                timeout=10
            )
            
            if response.status_code == 200:
                articles = response.json().get('articles', [])
                
                for article in articles[:30]:  # –õ–∏–º–∏—Ç 30 –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –º–æ–Ω–µ—Ç—É
                    NewsArticle.objects.get_or_create(
                        url=article['url'],
                        defaults={
                            'coin': coin,
                            'title': article.get('title', '')[:200],
                            'description': article.get('description', '')[:500],
                            'source': article.get('source', {}).get('name', 'Unknown'),
                            'published_at': article['publishedAt'],
                            'news_type': 'financial'
                        }
                    )
                
                print(f"‚úÖ {coin.symbol}: {len(articles)} news collected")
            
            time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            
        except Exception as e:
            print(f"‚ùå Error collecting news for {coin.symbol}: {e}")
    
    return {'status': 'success'}



# ============================================
# –ó–ê–î–ê–ß–ò –°–û–ó–î–ê–ù–ò–Ø –ü–†–û–ì–ù–û–ó–ê
# ============================================  

# subscriptions/tasks.py

def compute_features_for_coin(coin):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –ù–û–í–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏
    """
    from django.utils import timezone
    from datetime import timedelta
    import numpy as np
    import pandas as pd
    
    now = timezone.now()
    
    # 1. –¶–ï–ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò (7 –¥–Ω–µ–π)
    prices_7d = list(
        CoinDailyStat.objects
        .filter(coin=coin, date__gte=now.date() - timedelta(days=7))
        .order_by('-date')
        .values_list('price', 'volume', flat=False)[:7]
    )
    
    if len(prices_7d) < 7:
        return None
    
    prices = [float(p[0]) for p in prices_7d]
    volumes = [float(p[1]) for p in prices_7d]
    
    avg_price_7d = np.mean(prices)
    volatility_7d = np.std(prices)
    price_trend_7d = ((prices[0] - prices[-1]) / prices[-1]) * 100
    avg_volume_7d = np.mean(volumes)
    
    # 2. –ù–û–í–û–°–¢–ò - –¢–ï–ö–£–©–ò–ô –ü–ï–†–ò–û–î (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è)
    news_current = NewsArticle.objects.filter(
        coin=coin,
        published_at__gte=now - timedelta(days=3)
    ).select_related('newssentiment')
    
    # 3. –ù–û–í–û–°–¢–ò - –ü–†–ï–î–´–î–£–©–ò–ô –ü–ï–†–ò–û–î (–¥–Ω–∏ -6 –¥–æ -3)
    news_previous = NewsArticle.objects.filter(
        coin=coin,
        published_at__gte=now - timedelta(days=6),
        published_at__lt=now - timedelta(days=3)
    ).select_related('newssentiment')
    
    # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
    news_count_current = news_current.count()
    sentiments_current = [
        n.newssentiment.sentiment_score 
        for n in news_current 
        if hasattr(n, 'newssentiment')
    ]
    avg_sentiment_current = np.mean(sentiments_current) if sentiments_current else 0
    positive_current = sum(1 for s in sentiments_current if s > 0.05)
    negative_current = sum(1 for s in sentiments_current if s < -0.05)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
    news_count_previous = news_previous.count()
    sentiments_previous = [
        n.newssentiment.sentiment_score 
        for n in news_previous 
        if hasattr(n, 'newssentiment')
    ]
    avg_sentiment_previous = np.mean(sentiments_previous) if sentiments_previous else 0
    positive_previous = sum(1 for s in sentiments_previous if s > 0.05)
    negative_previous = sum(1 for s in sentiments_previous if s < -0.05)
    
    # === –ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
    news_volume_change = news_count_current - news_count_previous
    sentiment_change = avg_sentiment_current - avg_sentiment_previous
    positive_change = positive_current - positive_previous
    negative_change = negative_current - negative_previous
    
    negative_spike = 1 if (negative_current > 5 and negative_change > 3) else 0
    positive_spike = 1 if (positive_current > 5 and positive_change > 3) else 0
    
    price_sentiment_alignment = price_trend_7d * avg_sentiment_current
    divergence = 1 if (price_trend_7d < -1 and avg_sentiment_current > 0.1) else 0
    
    features_dict = {
        'price_trend_7d': price_trend_7d,
        'volatility_7d': volatility_7d,
        'avg_volume_7d': avg_volume_7d,
        'avg_price_7d': avg_price_7d,
        'news_volume_change': float(news_volume_change),
        'sentiment_change': float(sentiment_change),
        'positive_change': float(positive_change),
        'negative_change': float(negative_change),
        'negative_spike': float(negative_spike),
        'positive_spike': float(positive_spike),
        'price_sentiment_alignment': float(price_sentiment_alignment),
        'divergence': float(divergence),
    }
    
    return pd.DataFrame([features_dict])


@shared_task
def generate_daily_predictions():
    """
    –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –¥–∞–Ω–Ω—ã—Ö
    """
    from django.utils import timezone
    import numpy as np
    import pandas as pd
    import joblib
    
    print(f"üîÆ Generating predictions at {timezone.now()}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, scaler –∏ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    try:
        model = joblib.load('subscriptions/ml_model.pkl')
        scaler = joblib.load('subscriptions/ml_scaler.pkl')
        feature_cols = joblib.load('subscriptions/feature_columns.pkl')
    except FileNotFoundError as e:
        print(f"‚ùå Model files not found: {e}")
        return {'error': 'Model not trained'}
    
    today = timezone.now().date()
    predictions_created = 0
    
    for coin in CoinSnapshot.objects.all():
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame)
            features_df = compute_features_for_coin(coin)
            
            if features_df is None:
                print(f"‚ö†Ô∏è  {coin.symbol}: insufficient data")
                continue
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            X = features_df[feature_cols]
            
            # === –ü–†–ò–ú–ï–ù–Ø–ï–ú –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï (warning –∏—Å—á–µ–∑–Ω–µ—Ç) ===
            X_scaled = scaler.transform(X)
            
            # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            predicted_change = model.predict(X_scaled)[0]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é —Ü–µ–Ω—É
            current_price = float(coin.price)
            predicted_price = current_price * (1 + predicted_change / 100)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑
            prediction, created = PricePrediction.objects.update_or_create(
                coin=coin,
                prediction_date=today,
                defaults={
                    'predicted_change_percent': predicted_change,
                    'predicted_price': predicted_price,
                    'current_price': current_price,
                    'model_version': '3.0'
                }
            )
            
            if created:
                predictions_created += 1
                
            emoji = "üü¢" if predicted_change > 0 else "üî¥"
            print(f"{emoji} {coin.symbol:>6}: {predicted_change:>+6.2f}% (${predicted_price:>10,.2f})")
            
        except Exception as e:
            print(f"‚ùå Error predicting {coin.symbol}: {e}")
            continue
    
    print(f"\n‚úÖ Generated {predictions_created} predictions")
    
    return {
        'status': 'success',
        'predictions_created': predictions_created,
        'timestamp': timezone.now().isoformat()
    }


@shared_task
def generate_daily_predictions_classifier():
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª–∏ –∏–∑ ml/models/
    """
    from django.utils import timezone
    import numpy as np
    import pandas as pd
    import joblib
    
    print(f"üîÆ Generating direction predictions at {timezone.now()}")
    
    # –ó–ê–ì–†–£–ñ–ê–ï–ú –ò–ó ml/models/
    try:
        print(f"üìÇ Loading models from: {ML_MODELS_DIR}")
        model = joblib.load(CLASSIFIER_MODEL_PATH)
        scaler = joblib.load(CLASSIFIER_SCALER_PATH)
        feature_cols = joblib.load(CLASSIFIER_FEATURES_PATH)
        print("‚úÖ Models loaded successfully")
    except FileNotFoundError as e:
        print(f"‚ùå Model files not found: {e}")
        print(f"   Expected location: {ML_MODELS_DIR}")
        return {'error': 'Classifier not trained', 'path': str(ML_MODELS_DIR)}
    
    today = timezone.now().date()
    predictions_created = 0
    predictions_updated = 0
    
    for coin in CoinSnapshot.objects.all():
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features_df = compute_features_for_coin(coin)
            
            if features_df is None:
                print(f"‚ö†Ô∏è  {coin.symbol}: insufficient data")
                continue
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            X = features_df[feature_cols]
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
            X_scaled = scaler.transform(X)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            direction_code = model.predict(X_scaled)[0]
            probability = model.predict_proba(X_scaled)[0]
            
            prob_down = float(probability[0])
            prob_up = float(probability[1])
            
            predicted_direction = 'UP' if direction_code == 1 else 'DOWN'
            confidence = max(prob_down, prob_up)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            if predicted_direction == 'UP':
                estimated_change = 1.5 * confidence
            else:
                estimated_change = -1.5 * confidence
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–æ—á–Ω—É—é —Ü–µ–Ω—É
            current_price = float(coin.price)
            estimated_price = current_price * (1 + estimated_change / 100)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑
            prediction, created = DirectionPrediction.objects.update_or_create(
                coin=coin,
                prediction_date=today,
                defaults={
                    'predicted_direction': predicted_direction,
                    'confidence_score': confidence,
                    'probability_up': prob_up,
                    'probability_down': prob_down,
                    'estimated_change_percent': estimated_change,
                    'current_price': current_price,
                    'estimated_price': estimated_price,
                    'model_version': 'classifier_v2'
                }
            )
            
            if created:
                predictions_created += 1
            else:
                predictions_updated += 1
            
            emoji = "üü¢" if predicted_direction == 'UP' else "üî¥"
            signal = prediction.signal_strength.upper()
            
            print(f"{emoji} {coin.symbol:>6}: {predicted_direction:>4} "
                  f"({confidence*100:>5.1f}% confident, {signal:>8}) ‚Üí {estimated_change:>+6.2f}%")
            
        except Exception as e:
            print(f"‚ùå Error predicting {coin.symbol}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n‚úÖ Generated {predictions_created} new predictions, updated {predictions_updated}")
    
    return {
        'status': 'success',
        'predictions_created': predictions_created,
        'predictions_updated': predictions_updated,
        'total': predictions_created + predictions_updated,
        'models_location': str(ML_MODELS_DIR),
        'timestamp': timezone.now().isoformat()
    }


@shared_task
def generate_model_report():
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –º–æ–¥–µ–ª–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ml/models/
    """
    import json
    from datetime import datetime
    
    report = {
        'generated_at': datetime.now().isoformat(),
        'model_version': 'classifier_v2',
        'model_type': 'Gradient Boosting Classifier',
        'models_location': str(ML_MODELS_DIR),
        
        'dataset': {
            'total_samples': 480,
            'train_samples': 384,
            'test_samples': 96,
            'date_range': '2025-09-26 to 2025-12-16',
            'cryptocurrencies': 9,
            'news_articles': 2088,
            'sentiment_analyzer': 'FinBERT (ProsusAI/finbert)'
        },
        
        'features': {
            'total': 6,
            'price_features': ['price_trend_7d', 'volatility_7d', 'avg_volume_7d', 'avg_price_7d'],
            'news_features': ['sentiment_change', 'price_sentiment_alignment']
        },
        
        'performance': {
            'train_accuracy': 0.7031,
            'test_accuracy': 0.5312,
            'improvement_over_baseline': '+3.1%',
            'auc_roc': 0.5371,
            'overfitting_gap': 0.172
        },
        
        'feature_importance': {
            'price_features': '95.3%',
            'news_features': '4.7%',
            'top_feature': 'avg_volume_7d (33.2%)'
        },
        
        'key_findings': [
            'Model achieves 53.1% accuracy, exceeding random baseline by 3.1%',
            'Price technical indicators dominate (95.3%) over news sentiment (4.7%)',
            'Model is conservative: high recall for DOWN (83%), low recall for UP (16%)',
            'FinBERT sentiment analysis provides marginal predictive power on daily granularity',
            'Suitable as a weak signal in ensemble trading strategies'
        ]
    }
    
    # –°–û–•–†–ê–ù–Ø–ï–ú –í ml/models/
    with open(MODEL_REPORT_PATH, 'w') as f:
        json.dump(report, f, indent=2)
    
    print("="*60)
    print("üìä MODEL PERFORMANCE REPORT")
    print("="*60)
    print(f"\nüéØ Test Accuracy: {report['performance']['test_accuracy']*100:.1f}%")
    print(f"   Improvement: {report['performance']['improvement_over_baseline']}")
    print(f"   AUC-ROC: {report['performance']['auc_roc']:.3f}")
    print(f"\nüíæ Report saved to: {MODEL_REPORT_PATH}")
    
    return report


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)





# –ü–æ–∫–∞ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–∫–∏, –æ–Ω–∞ –Ω–µ –∑–∞—Ä–∞–±–æ—Ç–∞–ª–∞ —É –º–µ–Ω—è, –ø–æ –∏–¥–µ–µ –¥–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –º–µ–Ω—è—Ç—å Celery
#  –Ω–∞ –¥—Ä—É–≥—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ä–∞—Å—Å—ã–ª–∫–∏
TELEGRAM_API_URL = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"

@shared_task
def send_price_updates():
    subscriptions = Subscription.objects.select_related("user", "coin").all()
    user_messages = {}

    for sub in subscriptions:
        user_id = sub.user.telegram_id
        coin = sub.coin
        text = f"{coin.name} ({coin.symbol}): ${coin.price:.2f}"
        user_messages.setdefault(user_id, []).append(text)

    for user_id, messages in user_messages.items():
        full_message = "–í–∞—à–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞–º:\n\n" + "\n".join(messages)
        try:
            response = requests.post(TELEGRAM_API_URL, data={
                "chat_id": user_id,
                "text": full_message
            }, timeout=10)
            if not response.ok:
                print(f"[Telegram] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ {user_id}: {response.text}")
        except Exception as e:
            print(f"[Celery] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}: {e}")

