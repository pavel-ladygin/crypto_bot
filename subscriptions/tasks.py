# subscriptions/tasks.py

"""
Celery –∑–∞–¥–∞—á–∏ –¥–ª—è –∫—Ä–∏–ø—Ç–æ-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏

–ú–æ–¥—É–ª–∏:
1. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö (CoinGecko, NewsAPI)
2. –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (FinBERT)
3. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
"""

import logging
import os
import time
import json
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import requests
import joblib

from celery import shared_task
from django.db import transaction
from django.utils import timezone

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, 
    classification_report, 
    roc_auc_score, 
    confusion_matrix
)

from .models import (
    CoinSnapshot, 
    CoinDailyStat, 
    NewsArticle, 
    NewsSentiment, 
    DirectionPrediction
)


# ============================================
# –ü–£–¢–ò –ö –ú–û–î–ï–õ–Ø–ú ML
# ============================================

BASE_DIR = Path(__file__).resolve().parent.parent
ML_MODELS_DIR = BASE_DIR / 'ml' / 'models'

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
ML_MODELS_DIR.mkdir(parents=True, exist_ok=True)

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –º–æ–¥–µ–ª–µ–π
CLASSIFIER_MODEL_PATH = ML_MODELS_DIR / 'ml_classifier.pkl'
CLASSIFIER_SCALER_PATH = ML_MODELS_DIR / 'ml_classifier_scaler.pkl'
CLASSIFIER_FEATURES_PATH = ML_MODELS_DIR / 'classifier_features.pkl'

TRAINING_DATA_PATH = ML_MODELS_DIR / 'classification_data.csv'
MODEL_REPORT_PATH = ML_MODELS_DIR / 'model_report.json'


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================
# 1. –°–ë–û–† –î–ê–ù–ù–´–•
# ============================================

@shared_task
def update_coin_snapshots():
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–µ–∫—É—â–∏–µ —Ü–µ–Ω—ã –∏ –¥–∞–Ω–Ω—ã–µ –º–æ–Ω–µ—Ç –∏–∑ CoinGecko API
    """
    url = "https://api.coingecko.com/api/v3/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": 20,
        "page": 1,
        "sparkline": "false"
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        coins_data = response.json()
        
        with transaction.atomic():
            for coin in coins_data:
                CoinSnapshot.objects.update_or_create(
                    coingecko_id=coin["id"],
                    defaults={
                        "name": coin["name"],
                        "symbol": coin["symbol"],
                        "price": coin["current_price"],
                        "market_cap": coin.get("market_cap")
                    }
                )
        
        print(f"[{datetime.now()}] ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ {len(coins_data)} –º–æ–Ω–µ—Ç")
        return f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {len(coins_data)} –º–æ–Ω–µ—Ç"
        
    except requests.RequestException as e:
        print(f"[{datetime.now()}] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
        return f"–û—à–∏–±–∫–∞: {e}"


@shared_task
def collect_historical_prices(days=30):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–Ω–µ–≤–Ω—ã–µ —Ü–µ–Ω—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
    """
    coins = CoinSnapshot.objects.all()[:10]
    
    for coin in coins:
        try:
            url = f"https://api.coingecko.com/api/v3/coins/{coin.coingecko_id}/market_chart"
            params = {
                "vs_currency": "usd",
                "days": days,
                "interval": "daily"
            }
            
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            prices = data.get("prices", [])
            market_caps = data.get("market_caps", [])
            volumes = data.get("total_volumes", [])
            
            for i in range(len(prices)):
                timestamp = prices[i][0] / 1000
                date = datetime.utcfromtimestamp(timestamp).date()
                price = prices[i][1]
                market_cap = market_caps[i][1] if i < len(market_caps) else None
                volume = volumes[i][1] if i < len(volumes) else None
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
                prev_price = prices[i-1][1] if i > 0 else price
                price_change_percent = ((price - prev_price) / prev_price) * 100 if prev_price else 0
                
                CoinDailyStat.objects.update_or_create(
                    coin=coin,
                    date=date,
                    defaults={
                        "price": price,
                        "market_cap": market_cap,
                        "volume": volume,
                        "price_change_percent": price_change_percent
                    }
                )
            
            print(f"‚úÖ {coin.symbol.upper()} - –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(prices)} –¥–Ω–µ–π")
            time.sleep(60)  # Rate limiting
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {coin.symbol}: {e}")
            continue
    
    return f"–°–æ–±—Ä–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(coins)} –º–æ–Ω–µ—Ç"


@shared_task
def collect_historical_news(days=30):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π —á–µ—Ä–µ–∑ NewsAPI.org
    """
    from newsapi import NewsApiClient
    
    NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY')
    if not NEWSAPI_KEY:
        return "NEWSAPI_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω"
    
    newsapi = NewsApiClient(api_key=NEWSAPI_KEY)
    coins = CoinSnapshot.objects.order_by('-market_cap')[:10]
    total_articles = 0
    
    for coin in coins:
        try:
            queries = [
                f"{coin.name} cryptocurrency",
                f"{coin.symbol.upper()} price",
                f"{coin.name} news"
            ]
            
            from_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
            to_date = datetime.now().strftime('%Y-%m-%d')
            
            for query in queries:
                try:
                    all_articles = newsapi.get_everything(
                        q=query,
                        from_param=from_date,
                        to=to_date,
                        language='en',
                        sort_by='publishedAt',
                        page_size=100
                    )
                    
                    articles = all_articles.get('articles', [])
                    saved_count = 0
                    
                    for article in articles:
                        try:
                            if not article.get('url') or not article.get('title'):
                                continue
                            
                            published_str = article.get('publishedAt', '')
                            if published_str:
                                published_at = datetime.strptime(published_str, '%Y-%m-%dT%H:%M:%SZ')
                            else:
                                continue
                            
                            obj, created = NewsArticle.objects.get_or_create(
                                url=article['url'],
                                defaults={
                                    'coin': coin,
                                    'title': article['title'][:500],
                                    'description': article.get('description', '')[:1000] if article.get('description') else '',
                                    'source': article.get('source', {}).get('name', 'Unknown'),
                                    'published_at': published_at,
                                    'news_type': 'financial'
                                }
                            )
                            
                            if created:
                                saved_count += 1
                        except Exception:
                            continue
                    
                    total_articles += saved_count
                    print(f"  '{query}' - {saved_count} –Ω–æ–≤—ã—Ö")
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–ª—è '{query}': {e}")
                    continue
            
            print(f"‚úÖ {coin.symbol.upper()} - —Å–æ–±—Ä–∞–Ω–æ {total_articles} –Ω–æ–≤–æ—Å—Ç–µ–π")
            time.sleep(2)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {coin.symbol}: {e}")
            continue
    
    return f"–°–æ–±—Ä–∞–Ω–æ {total_articles} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"


# ============================================
# 2. –ê–ù–ê–õ–ò–ó –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò (FinBERT)
# ============================================

def analyze_with_finbert(text):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é FinBERT
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    import torch
    
    model_name = "ProsusAI/finbert"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    # Tokenize
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    
    # Predict
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
    # FinBERT classes: positive, negative, neutral
    positive_score = probs[0][0].item()
    negative_score = probs[0][1].item()
    neutral_score = probs[0][2].item()
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ [-1, 1] scale
    sentiment_score = positive_score - negative_score
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫—É
    max_idx = probs[0].argmax().item()
    labels = ['positive', 'negative', 'neutral']
    sentiment_label = labels[max_idx]
    confidence = probs[0][max_idx].item()
    
    return {
        'sentiment_score': sentiment_score,
        'sentiment_label': sentiment_label,
        'confidence': confidence
    }


@shared_task
def analyze_all_sentiment():
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å FinBERT
    """
    articles = NewsArticle.objects.filter(newssentiment__isnull=True)
    total_articles = articles.count()
    
    if total_articles == 0:
        return "–í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ —É–∂–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã"
    
    print(f"üí≠ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å {total_articles} –Ω–æ–≤–æ—Å—Ç–µ–π —Å FinBERT...")
    
    analyzed_count = 0
    for article in articles:
        try:
            text = f"{article.title}. {article.description or ''}"
            result = analyze_with_finbert(text)
            
            NewsSentiment.objects.create(
                article=article,
                sentiment_score=result['sentiment_score'],
                sentiment_label=result['sentiment_label'],
                confidence=result['confidence']
            )
            
            analyzed_count += 1
            
            if analyzed_count % 50 == 0:
                print(f"  –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {analyzed_count}/{total_articles}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–∞—Ç—å–∏ {article.id}: {e}")
            continue
    
    print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {analyzed_count} –∏–∑ {total_articles} —Å—Ç–∞—Ç–µ–π")
    return f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {analyzed_count} —Å—Ç–∞—Ç–µ–π"


# ============================================
# 3. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø
# ============================================

@shared_task
def prepare_classification_dataset():
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ml/models/classification_data.csv
    """
    data = []
    
    for coin in CoinSnapshot.objects.all():
        print(f"Processing {coin.symbol}...")
        
        daily_stats = list(
            CoinDailyStat.objects
            .filter(coin=coin)
            .order_by('date')
            .values('date', 'price', 'volume', 'market_cap')
        )
        
        if len(daily_stats) < 8:
            continue
        
        for i in range(7, len(daily_stats) - 1):
            current_day = daily_stats[i]
            next_day = daily_stats[i + 1]
            
            # TARGET: 0 = down, 1 = up
            price_current = float(current_day['price'])
            price_next = float(next_day['price'])
            price_change_percent = ((price_next - price_current) / price_current) * 100
            
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —à—É–º (<0.5%)
            if abs(price_change_percent) < 0.5:
                continue
            
            target = 1 if price_change_percent > 0 else 0
            
            # === –¶–ï–ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò (7 –¥–Ω–µ–π) ===
            past_7_days = daily_stats[i-6:i+1]
            prices_7d = [float(d['price']) for d in past_7_days]
            volumes_7d = [float(d['volume']) for d in past_7_days]
            
            avg_price_7d = np.mean(prices_7d)
            volatility_7d = np.std(prices_7d)
            price_trend_7d = ((prices_7d[-1] - prices_7d[0]) / prices_7d[0]) * 100
            avg_volume_7d = np.mean(volumes_7d)
            
            # === –ù–û–í–û–°–¢–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò ===
            date_3d_ago = current_day['date'] - timedelta(days=3)
            date_6d_ago = current_day['date'] - timedelta(days=6)
            
            # –¢–µ–∫—É—â–∏–π –ø–µ—Ä–∏–æ–¥ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è)
            news_current = NewsArticle.objects.filter(
                coin=coin,
                published_at__date__gte=date_3d_ago,
                published_at__date__lte=current_day['date']
            ).select_related('newssentiment')
            
            # –ü—Ä–µ–¥—ã–¥—É—â–∏–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏ -6 –¥–æ -3)
            news_previous = NewsArticle.objects.filter(
                coin=coin,
                published_at__date__gte=date_6d_ago,
                published_at__date__lt=date_3d_ago
            ).select_related('newssentiment')
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
            news_count_current = news_current.count()
            sentiments_current = [
                n.newssentiment.sentiment_score
                for n in news_current
                if hasattr(n, 'newssentiment')
            ]
            
            avg_sentiment_current = np.mean(sentiments_current) if sentiments_current else 0
            positive_current = sum(1 for s in sentiments_current if s > 0.05)
            negative_current = sum(1 for s in sentiments_current if s < -0.05)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
            news_count_previous = news_previous.count()
            sentiments_previous = [
                n.newssentiment.sentiment_score
                for n in news_previous
                if hasattr(n, 'newssentiment')
            ]
            
            avg_sentiment_previous = np.mean(sentiments_previous) if sentiments_previous else 0
            positive_previous = sum(1 for s in sentiments_previous if s > 0.05)
            negative_previous = sum(1 for s in sentiments_previous if s < -0.05)
            
            # === –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò ===
            news_volume_change = news_count_current - news_count_previous
            sentiment_change = avg_sentiment_current - avg_sentiment_previous
            positive_change = positive_current - positive_previous
            negative_change = negative_current - negative_previous
            
            # –í—Å–ø–ª–µ—Å–∫–∏
            negative_spike = 1 if (negative_current > 5 and negative_change > 3) else 0
            positive_spike = 1 if (positive_current > 5 and positive_change > 3) else 0
            
            # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
            price_sentiment_alignment = price_trend_7d * avg_sentiment_current
            divergence = 1 if (price_trend_7d < -1 and avg_sentiment_current > 0.1) else 0
            
            data.append({
                'coin': coin.symbol,
                'date': current_day['date'],
                'target': target,
                'price_change_percent': price_change_percent,
                
                # –¶–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                'price_trend_7d': price_trend_7d,
                'volatility_7d': volatility_7d,
                'avg_volume_7d': avg_volume_7d,
                'avg_price_7d': avg_price_7d,
                
                # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                'news_volume_change': float(news_volume_change),
                'sentiment_change': float(sentiment_change),
                'positive_change': float(positive_change),
                'negative_change': float(negative_change),
                'negative_spike': float(negative_spike),
                'positive_spike': float(positive_spike),
                'price_sentiment_alignment': float(price_sentiment_alignment),
                'divergence': float(divergence),
            })
    
    df = pd.DataFrame(data)
    
    up_count = (df['target'] == 1).sum()
    down_count = (df['target'] == 0).sum()
    
    print(f"\n‚úÖ Dataset created: {len(df)} samples")
    print(f"üìä Class distribution:")
    print(f"   UP (1):   {up_count} ({up_count/len(df)*100:.1f}%)")
    print(f"   DOWN (0): {down_count} ({down_count/len(df)*100:.1f}%)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ml/models/
    df.to_csv(TRAINING_DATA_PATH, index=False)
    print(f"üíæ Saved to: {TRAINING_DATA_PATH}")
    
    return {
        'total_samples': len(df),
        'up_count': int(up_count),
        'down_count': int(down_count),
        'saved_to': str(TRAINING_DATA_PATH)
    }


# ============================================
# 4. –û–ë–£–ß–ï–ù–ò–ï –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê
# ============================================

@shared_task
def train_classification_model_v2():
    """
    –û–±—É—á–∞–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞ (UP/DOWN)
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ –≤ ml/models/
    """
    print(f"üìÇ Loading data from: {TRAINING_DATA_PATH}")
    df = pd.read_csv(TRAINING_DATA_PATH)
    
    print(f"üìä Dataset: {len(df)} samples")
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤)
    feature_cols = [
        'price_trend_7d', 
        'volatility_7d', 
        'avg_volume_7d', 
        'avg_price_7d',
        'sentiment_change', 
        'price_sentiment_alignment',
    ]
    
    print(f"üéØ Using {len(feature_cols)} features")
    
    X = df[feature_cols]
    y = df['target']
    
    # Temporal split (80/20)
    df_sorted = df.sort_values('date')
    split_idx = int(len(df_sorted) * 0.8)
    
    train_df = df_sorted.iloc[:split_idx]
    test_df = df_sorted.iloc[split_idx:]
    
    X_train = train_df[feature_cols]
    y_train = train_df['target']
    X_test = test_df[feature_cols]
    y_test = test_df['target']
    
    print(f"\nüì¶ Train: {len(train_df)} samples")
    print(f"   UP: {(train_df['target']==1).sum()}, DOWN: {(train_df['target']==0).sum()}")
    print(f"üì¶ Test: {len(test_df)} samples")
    print(f"   UP: {(test_df['target']==1).sum()}, DOWN: {(test_df['target']==0).sum()}")
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –ú–æ–¥–µ–ª—å
    model = GradientBoostingClassifier(
        n_estimators=30,
        learning_rate=0.1,
        max_depth=3,
        min_samples_split=30,
        min_samples_leaf=15,
        subsample=0.7,
        max_features='sqrt',
        random_state=42,
        verbose=0
    )
    
    print("\nüîß Training classifier...")
    model.fit(X_train_scaled, y_train)
    
    # –û—Ü–µ–Ω–∫–∞
    train_pred = model.predict(X_train_scaled)
    test_pred = model.predict(X_test_scaled)
    
    train_pred_proba = model.predict_proba(X_train_scaled)[:, 1]
    test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    train_acc = accuracy_score(y_train, train_pred)
    test_acc = accuracy_score(y_test, test_pred)
    
    train_auc = roc_auc_score(y_train, train_pred_proba)
    test_auc = roc_auc_score(y_test, test_pred_proba)
    
    print("\n" + "="*60)
    print("üìà CLASSIFICATION PERFORMANCE")
    print("="*60)
    print(f"Train Accuracy: {train_acc:.4f} ({train_acc*100:.1f}%)")
    print(f"Test Accuracy:  {test_acc:.4f} ({test_acc*100:.1f}%)  {'‚úÖ' if test_acc > 0.52 else '‚ö†Ô∏è'}")
    print(f"Train AUC-ROC:  {train_auc:.4f}")
    print(f"Test AUC-ROC:   {test_auc:.4f}  {'‚úÖ' if test_auc > 0.55 else '‚ö†Ô∏è'}")
    print(f"\nüìä Comparison:")
    print(f"   Baseline (random):     50.0%")
    print(f"   Your model:           {test_acc*100:.1f}%")
    print(f"   Improvement:          +{(test_acc - 0.5)*100:.1f}%")
    print(f"   Overfitting gap:      {(train_acc - test_acc)*100:.1f}%  {'‚úÖ' if (train_acc - test_acc) < 0.15 else '‚ö†Ô∏è'}")
    
    # Confusion matrix
    cm = confusion_matrix(y_test, test_pred)
    print("\n" + "="*60)
    print("üìã CONFUSION MATRIX (Test Set)")
    print("="*60)
    print(f"                Predicted")
    print(f"              DOWN    UP")
    print(f"Actual DOWN    {cm[0][0]:3d}   {cm[0][1]:3d}")
    print(f"       UP      {cm[1][0]:3d}   {cm[1][1]:3d}")
    
    # Classification report
    print("\n" + "="*60)
    print("üìã DETAILED METRICS")
    print("="*60)
    print(classification_report(y_test, test_pred, target_names=['DOWN', 'UP']))
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n" + "="*60)
    print("üîù FEATURE IMPORTANCE")
    print("="*60)
    for idx, row in feature_importance.iterrows():
        bar = "‚ñà" * int(row['importance'] * 100)
        category = "üí∞" if any(p in row['feature'] for p in ['price', 'volume', 'volatility']) else "üì∞"
        print(f"{category} {row['feature']:.<35} {row['importance']*100:>5.1f}% {bar}")
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
    price_features = ['avg_price_7d', 'volatility_7d', 'price_trend_7d', 'avg_volume_7d']
    news_features = [f for f in feature_cols if f not in price_features]
    
    price_importance = feature_importance[feature_importance['feature'].isin(price_features)]['importance'].sum()
    news_importance = feature_importance[feature_importance['feature'].isin(news_features)]['importance'].sum()
    
    print("\n" + "="*60)
    print("üìä FEATURE GROUPS")
    print("="*60)
    print(f"üí∞ Price features:  {price_importance*100:>5.1f}%")
    print(f"üì∞ News features:   {news_importance*100:>5.1f}%")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ml/models/
    joblib.dump(model, CLASSIFIER_MODEL_PATH)
    joblib.dump(scaler, CLASSIFIER_SCALER_PATH)
    joblib.dump(feature_cols, CLASSIFIER_FEATURES_PATH)
    
    print("\n" + "="*60)
    print("üíæ MODEL SAVED")
    print("="*60)
    print(f"   Model:    {CLASSIFIER_MODEL_PATH}")
    print(f"   Scaler:   {CLASSIFIER_SCALER_PATH}")
    print(f"   Features: {CLASSIFIER_FEATURES_PATH}")
    
    return {
        'train_acc': float(train_acc),
        'test_acc': float(test_acc),
        'train_auc': float(train_auc),
        'test_auc': float(test_auc),
        'improvement': float((test_acc - 0.5) * 100),
        'overfitting_gap': float((train_acc - test_acc) * 100),
        'price_importance': float(price_importance),
        'news_importance': float(news_importance),
        'confusion_matrix': cm.tolist(),
        'saved_to': {
            'model': str(CLASSIFIER_MODEL_PATH),
            'scaler': str(CLASSIFIER_SCALER_PATH),
            'features': str(CLASSIFIER_FEATURES_PATH)
        }
    }


# ============================================
# 5. –í–´–ß–ò–°–õ–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø
# ============================================

def compute_features_for_coin(coin):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–Ω–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–ª–∏ None –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
    """
    now = timezone.now()
    
    # 1. –¶–ï–ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π)
    prices_7d = list(
        CoinDailyStat.objects
        .filter(coin=coin, date__gte=now.date() - timedelta(days=7))
        .order_by('-date')
        .values_list('price', 'volume', flat=False)[:7]
    )
    
    if len(prices_7d) < 7:
        return None
    
    prices = [float(p[0]) for p in prices_7d]
    volumes = [float(p[1]) for p in prices_7d]
    
    avg_price_7d = np.mean(prices)
    volatility_7d = np.std(prices)
    price_trend_7d = ((prices[0] - prices[-1]) / prices[-1]) * 100
    avg_volume_7d = np.mean(volumes)
    
    # 2. –ù–û–í–û–°–¢–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    # –¢–µ–∫—É—â–∏–π –ø–µ—Ä–∏–æ–¥ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è)
    news_current = NewsArticle.objects.filter(
        coin=coin,
        published_at__gte=now - timedelta(days=3)
    ).select_related('newssentiment')
    
    # –ü—Ä–µ–¥—ã–¥—É—â–∏–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏ -6 –¥–æ -3)
    news_previous = NewsArticle.objects.filter(
        coin=coin,
        published_at__gte=now - timedelta(days=6),
        published_at__lt=now - timedelta(days=3)
    ).select_related('newssentiment')
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
    news_count_current = news_current.count()
    sentiments_current = [
        n.newssentiment.sentiment_score
        for n in news_current
        if hasattr(n, 'newssentiment')
    ]
    
    avg_sentiment_current = np.mean(sentiments_current) if sentiments_current else 0
    positive_current = sum(1 for s in sentiments_current if s > 0.05)
    negative_current = sum(1 for s in sentiments_current if s < -0.05)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
    news_count_previous = news_previous.count()
    sentiments_previous = [
        n.newssentiment.sentiment_score
        for n in news_previous
        if hasattr(n, 'newssentiment')
    ]
    
    avg_sentiment_previous = np.mean(sentiments_previous) if sentiments_previous else 0
    positive_previous = sum(1 for s in sentiments_previous if s > 0.05)
    negative_previous = sum(1 for s in sentiments_previous if s < -0.05)
    
    # 3. –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò
    news_volume_change = news_count_current - news_count_previous
    sentiment_change = avg_sentiment_current - avg_sentiment_previous
    positive_change = positive_current - positive_previous
    negative_change = negative_current - negative_previous
    
    negative_spike = 1 if (negative_current > 5 and negative_change > 3) else 0
    positive_spike = 1 if (positive_current > 5 and positive_change > 3) else 0
    
    price_sentiment_alignment = price_trend_7d * avg_sentiment_current
    divergence = 1 if (price_trend_7d < -1 and avg_sentiment_current > 0.1) else 0
    
    features_dict = {
        'price_trend_7d': price_trend_7d,
        'volatility_7d': volatility_7d,
        'avg_volume_7d': avg_volume_7d,
        'avg_price_7d': avg_price_7d,
        'news_volume_change': float(news_volume_change),
        'sentiment_change': float(sentiment_change),
        'positive_change': float(positive_change),
        'negative_change': float(negative_change),
        'negative_spike': float(negative_spike),
        'positive_spike': float(positive_spike),
        'price_sentiment_alignment': float(price_sentiment_alignment),
        'divergence': float(divergence),
    }
    
    return pd.DataFrame([features_dict])


# ============================================
# 6. –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–†–û–ì–ù–û–ó–û–í
# ============================================

@shared_task
def generate_daily_predictions_classifier():
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –º–æ–Ω–µ—Ç
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑ ml/models/
    """
    print(f"üîÆ Generating direction predictions at {timezone.now()}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –∏–∑ ml/models/
    try:
        print(f"üìÇ Loading models from: {ML_MODELS_DIR}")
        model = joblib.load(CLASSIFIER_MODEL_PATH)
        scaler = joblib.load(CLASSIFIER_SCALER_PATH)
        feature_cols = joblib.load(CLASSIFIER_FEATURES_PATH)
        print("‚úÖ Models loaded successfully")
    except FileNotFoundError as e:
        print(f"‚ùå Model files not found: {e}")
        print(f"   Expected location: {ML_MODELS_DIR}")
        return {'error': 'Classifier not trained', 'path': str(ML_MODELS_DIR)}
    
    today = timezone.now().date()
    predictions_created = 0
    predictions_updated = 0
    
    for coin in CoinSnapshot.objects.all():
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features_df = compute_features_for_coin(coin)
            
            if features_df is None:
                print(f"‚ö†Ô∏è  {coin.symbol}: insufficient data")
                continue
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            X = features_df[feature_cols]
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
            X_scaled = scaler.transform(X)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            direction_code = model.predict(X_scaled)[0]
            probability = model.predict_proba(X_scaled)[0]
            
            prob_down = float(probability[0])
            prob_up = float(probability[1])
            
            predicted_direction = 'UP' if direction_code == 1 else 'DOWN'
            confidence = max(prob_down, prob_up)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
            if predicted_direction == 'UP':
                estimated_change = 1.5 * confidence
            else:
                estimated_change = -1.5 * confidence
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–æ—á–Ω—É—é —Ü–µ–Ω—É
            current_price = float(coin.price)
            estimated_price = current_price * (1 + estimated_change / 100)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑
            prediction, created = DirectionPrediction.objects.update_or_create(
                coin=coin,
                prediction_date=today,
                defaults={
                    'predicted_direction': predicted_direction,
                    'confidence_score': confidence,
                    'probability_up': prob_up,
                    'probability_down': prob_down,
                    'estimated_change_percent': estimated_change,
                    'current_price': current_price,
                    'estimated_price': estimated_price,
                    'model_version': 'classifier_v2'
                }
            )
            
            if created:
                predictions_created += 1
            else:
                predictions_updated += 1
            
            emoji = "üü¢" if predicted_direction == 'UP' else "üî¥"
            signal = prediction.signal_strength.upper()
            
            print(f"{emoji} {coin.symbol:>6}: {predicted_direction:>4} "
                  f"({confidence*100:>5.1f}% confident, {signal:>8}) ‚Üí {estimated_change:>+6.2f}%")
            
        except Exception as e:
            print(f"‚ùå Error predicting {coin.symbol}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n‚úÖ Generated {predictions_created} new predictions, updated {predictions_updated}")
    
    return {
        'status': 'success',
        'predictions_created': predictions_created,
        'predictions_updated': predictions_updated,
        'total': predictions_created + predictions_updated,
        'models_location': str(ML_MODELS_DIR),
        'timestamp': timezone.now().isoformat()
    }


# ============================================
# 7. –û–¢–ß–ï–¢ –û –ú–û–î–ï–õ–ò
# ============================================

@shared_task
def generate_model_report():
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ml/models/model_report.json
    """
    report = {
        'generated_at': datetime.now().isoformat(),
        'model_version': 'classifier_v2',
        'model_type': 'Gradient Boosting Classifier',
        'models_location': str(ML_MODELS_DIR),
        
        'dataset': {
            'total_samples': 480,
            'train_samples': 384,
            'test_samples': 96,
            'date_range': '2025-09-26 to 2025-12-16',
            'cryptocurrencies': 9,
            'news_articles': 2088,
            'sentiment_analyzer': 'FinBERT (ProsusAI/finbert)'
        },
        
        'features': {
            'total': 6,
            'price_features': ['price_trend_7d', 'volatility_7d', 'avg_volume_7d', 'avg_price_7d'],
            'news_features': ['sentiment_change', 'price_sentiment_alignment']
        },
        
        'performance': {
            'train_accuracy': 0.7031,
            'test_accuracy': 0.5312,
            'improvement_over_baseline': '+3.1%',
            'auc_roc': 0.5371,
            'overfitting_gap': 0.172
        },
        
        'feature_importance': {
            'price_features': '95.3%',
            'news_features': '4.7%',
            'top_feature': 'avg_volume_7d (33.2%)'
        },
        
        'key_findings': [
            'Model achieves 53.1% accuracy, exceeding random baseline by 3.1%',
            'Price technical indicators dominate (95.3%) over news sentiment (4.7%)',
            'Model is conservative: high recall for DOWN (83%), low recall for UP (16%)',
            'FinBERT sentiment analysis provides marginal predictive power on daily granularity',
            'Suitable as a weak signal in ensemble trading strategies'
        ]
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ml/models/
    with open(MODEL_REPORT_PATH, 'w') as f:
        json.dump(report, f, indent=2)
    
    print("="*60)
    print("üìä MODEL PERFORMANCE REPORT")
    print("="*60)
    print(f"\nüéØ Test Accuracy: {report['performance']['test_accuracy']*100:.1f}%")
    print(f"   Improvement: {report['performance']['improvement_over_baseline']}")
    print(f"   AUC-ROC: {report['performance']['auc_roc']:.3f}")
    print(f"\nüíæ Report saved to: {MODEL_REPORT_PATH}")
    
    return report


# ============================================
# 8. –ê–í–¢–û–ú–ê–¢–ò–ó–ê–¶–ò–Ø (–î–õ–Ø CELERY BEAT)
# ============================================

@shared_task
def update_daily_data():
    """
    –ï–∂–µ–¥–Ω–µ–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:
    1. –û–±–Ω–æ–≤–ª—è–µ—Ç —Ü–µ–Ω—ã –º–æ–Ω–µ—Ç
    2. –°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã
    3. –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏
    4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã
    """
    print(f"üîÑ Daily data update started at {timezone.now()}")
    
    # 1. –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ —Ü–µ–Ω—ã
    update_coin_snapshots()
    
    # 2. –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–Ω—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –¥–Ω—è
    collect_historical_prices(days=2)
    
    # 3. –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å
    collect_historical_news(days=1)
    
    # 4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    analyze_all_sentiment()
    
    # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã
    generate_daily_predictions_classifier()
    
    print(f"‚úÖ Daily data update completed at {timezone.now()}")
    
    return {
        'status': 'success',
        'timestamp': timezone.now().isoformat()
    }


# subscriptions/tasks.py (–¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞)

# ============================================
# 9. TELEGRAM –†–ê–°–°–´–õ–ö–ò
# ============================================

@shared_task
def send_daily_predictions_to_users():
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –≤—Å–µ–º –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º
    –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 10:00 MSK (07:00 UTC)
    """
    import asyncio
    from aiogram import Bot
    from subscriptions.models import Subscription, BotUser, DirectionPrediction
    from datetime import date
    import os
    
    TG_TOKEN = os.getenv("TG_TOKEN")
    if not TG_TOKEN:
        print("‚ùå TG_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return {"error": "TG_TOKEN not found"}
    
    bot = Bot(token=TG_TOKEN)
    today = date.today()
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∫–∏
    subscriptions = Subscription.objects.select_related('user', 'coin').all()
    
    sent_count = 0
    error_count = 0
    
    print(f"üì§ –ù–∞—á–∏–Ω–∞—é —Ä–∞—Å—Å—ã–ª–∫—É –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –Ω–∞ {today}...")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ–¥–ø–∏—Å–∫–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
    users_coins = {}
    for sub in subscriptions:
        if sub.user.telegram_id not in users_coins:
            users_coins[sub.user.telegram_id] = []
        users_coins[sub.user.telegram_id].append(sub.coin)
    
    async def send_predictions():
        nonlocal sent_count, error_count
        
        for telegram_id, coins in users_coins.items():
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                message_parts = [
                    "üîÆ <b>–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! –í–∞—à–∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Å–µ–≥–æ–¥–Ω—è:</b>\n",
                    "‚ïê" * 30 + "\n"
                ]
                
                for coin in coins:
                    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è
                    prediction = DirectionPrediction.objects.filter(
                        coin=coin,
                        prediction_date=today
                    ).first()
                    
                    if not prediction:
                        message_parts.append(
                            f"\nüí∞ <b>{coin.name} ({coin.symbol.upper()})</b>\n"
                            f"‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑ –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤\n"
                        )
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—É –≤—á–µ—Ä–∞—à–Ω–µ–≥–æ –¥–Ω—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
                    yesterday_stat = CoinDailyStat.objects.filter(
                        coin=coin,
                        date=today - timedelta(days=1)
                    ).first()
                    
                    if yesterday_stat:
                        price_yesterday = float(yesterday_stat.price)
                        price_current = float(prediction.current_price)
                        daily_change = ((price_current - price_yesterday) / price_yesterday) * 100
                        daily_change_emoji = "üü¢" if daily_change > 0 else "üî¥"
                    else:
                        daily_change = None
                        daily_change_emoji = "‚ö™"
                    
                    # Emoji –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞
                    direction_emoji = "üü¢ ‚ÜóÔ∏è" if prediction.predicted_direction == 'UP' else "üî¥ ‚ÜòÔ∏è"
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –±–ª–æ–∫ –¥–ª—è –º–æ–Ω–µ—Ç—ã
                    message_parts.append(
                        f"\nüí∞ <b>{coin.name} ({coin.symbol.upper()})</b>\n"
                        f"üíµ –¶–µ–Ω–∞: ${prediction.current_price:,.2f}"
                    )
                    
                    if daily_change is not None:
                        message_parts.append(
                            f" ({daily_change_emoji} {daily_change:+.2f}% –∑–∞ 24—á)\n"
                        )
                    else:
                        message_parts.append("\n")
                    
                    message_parts.append(
                        f"{direction_emoji} <b>–ü—Ä–æ–≥–Ω–æ–∑:</b> {prediction.predicted_direction} "
                        f"({prediction.estimated_change_percent:+.2f}%)\n"
                        f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction.confidence_score*100:.0f}%\n"
                        f"üìä –¶–µ–ª–µ–≤–∞—è —Ü–µ–Ω–∞: ${prediction.estimated_price:,.2f}\n"
                    )
                
                message_parts.append(
                    f"\n{'‚ïê' * 30}\n"
                    f"<i>‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–æ—Å—è—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä</i>\n\n"
                    f"üîÆ –ü–æ–¥—Ä–æ–±–Ω–µ–µ: /predictions"
                )
                
                message = "".join(message_parts)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                await bot.send_message(
                    chat_id=telegram_id,
                    text=message,
                    parse_mode="HTML"
                )
                
                sent_count += 1
                print(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {telegram_id} ({len(coins)} –º–æ–Ω–µ—Ç)")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
                await asyncio.sleep(0.5)
                
            except Exception as e:
                error_count += 1
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {telegram_id}: {e}")
                continue
        
        await bot.session.close()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ—Ç–ø—Ä–∞–≤–∫—É
    asyncio.run(send_predictions())
    
    print(f"\n‚úÖ –†–∞—Å—Å—ã–ª–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    print(f"   –£—Å–ø–µ—à–Ω–æ: {sent_count}")
    print(f"   –û—à–∏–±–∫–∏: {error_count}")
    
    return {
        'status': 'success',
        'sent': sent_count,
        'errors': error_count,
        'timestamp': timezone.now().isoformat()
    }

# subscriptions/tasks.py

@shared_task
def send_test_prediction(telegram_id: int, coin_symbol: str):
    """
    –¢–µ—Å—Ç–æ–≤–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞ (HTTP API –≤–µ—Ä—Å–∏—è)
    """
    import requests
    import os
    from subscriptions.models import CoinSnapshot, DirectionPrediction, CoinDailyStat
    from datetime import date, timedelta
    
    print(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è {coin_symbol} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {telegram_id}")
    
    TG_TOKEN = os.getenv("TG_TOKEN")
    if not TG_TOKEN:
        print("‚ùå TG_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        return {"error": "TG_TOKEN not found"}
    
    today = date.today()
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–Ω–µ—Ç—É
        coin = CoinSnapshot.objects.get(symbol=coin_symbol.lower())
        print(f"‚úÖ –ú–æ–Ω–µ—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞: {coin.name}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑
        prediction = DirectionPrediction.objects.filter(
            coin=coin,
            prediction_date=today
        ).first()
        
        if not prediction:
            print(f"‚ùå –ü—Ä–æ–≥–Ω–æ–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è {coin_symbol} –Ω–∞ {today}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏
            url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
            data = {
                "chat_id": telegram_id,
                "text": f"‚ùå –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {coin_symbol.upper()} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ {today}\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /predictions",
                "parse_mode": "HTML"
            }
            response = requests.post(url, json=data, timeout=10)
            print(f"–û—Ç–≤–µ—Ç Telegram API: {response.status_code}")
            return {"error": "prediction not found", "date": str(today)}
        
        print(f"‚úÖ –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞–π–¥–µ–Ω: {prediction.predicted_direction} ({prediction.confidence_score*100:.0f}%)")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞ –¥–µ–Ω—å
        yesterday_stat = CoinDailyStat.objects.filter(
            coin=coin,
            date=today - timedelta(days=1)
        ).first()
        
        if yesterday_stat:
            price_yesterday = float(yesterday_stat.price)
            price_current = float(prediction.current_price)
            daily_change = ((price_current - price_yesterday) / price_yesterday) * 100
            daily_change_emoji = "üü¢" if daily_change > 0 else "üî¥"
            daily_change_text = f" ({daily_change_emoji} {daily_change:+.2f}% –∑–∞ 24—á)"
        else:
            daily_change_text = ""
        
        direction_emoji = "üü¢ ‚ÜóÔ∏è" if prediction.predicted_direction == 'UP' else "üî¥ ‚ÜòÔ∏è"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        message = (
            f"üîÆ <b>–¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑</b>\n"
            f"{'‚ïê' * 30}\n\n"
            f"üí∞ <b>{coin.name} ({coin.symbol.upper()})</b>\n"
            f"üíµ –¶–µ–Ω–∞: ${prediction.current_price:,.2f}{daily_change_text}\n"
            f"{direction_emoji} <b>–ü—Ä–æ–≥–Ω–æ–∑:</b> {prediction.predicted_direction} "
            f"({prediction.estimated_change_percent:+.2f}%)\n"
            f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction.confidence_score*100:.0f}%\n"
            f"üìä –¶–µ–ª–µ–≤–∞—è —Ü–µ–Ω–∞: ${prediction.estimated_price:,.2f}\n\n"
            f"<i>‚ö†Ô∏è –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ Celery</i>"
        )
        
        print(f"üìù –°–æ–æ–±—â–µ–Ω–∏–µ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ, –¥–ª–∏–Ω–∞: {len(message)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–µ—Ä–µ–∑ Telegram HTTP API
        url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
        data = {
            "chat_id": telegram_id,
            "text": message,
            "parse_mode": "HTML"
        }
        
        print(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ Telegram API...")
        response = requests.post(url, json=data, timeout=10)
        
        print(f"üì° –û—Ç–≤–µ—Ç –æ—Ç Telegram: {response.status_code}")
        
        if response.status_code == 200:
            print(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {telegram_id}")
            return {
                "status": "success",
                "telegram_id": telegram_id,
                "coin": coin_symbol,
                "message_sent": True
            }
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.text}")
            return {
                "error": "telegram_api_error",
                "status_code": response.status_code,
                "response": response.text
            }
            
    except CoinSnapshot.DoesNotExist:
        print(f"‚ùå –ú–æ–Ω–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {coin_symbol}")
        return {"error": "coin_not_found", "coin": coin_symbol}
        
    except Exception as e:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e), "type": type(e).__name__}
